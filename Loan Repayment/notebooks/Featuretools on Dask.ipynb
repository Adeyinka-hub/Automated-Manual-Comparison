{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../images/featuretools.png)\n",
    "\n",
    "# Featuretools Implementation with Dask\n",
    "\n",
    "A simple run of Deep Feature Synthesis from the Automated Loan Repayment notebook takes about 25 hours on an AWS machine with 64 GB of RAM! Clearly we need a better approach for practical implementations of calculating a large feature matrix.\n",
    "\n",
    "Featuretools does have support for parallel processing if you have multiple cores (which nearly every single laptop now does), but it currently sends the entire EntitySet to each process which means you might exhaust the memory on any one core. For example, that AWS machine has 8 GB per core, which might seem like a lot until you realize the EntitySet takes up about 11 GB and setting `n_jobs=-1` will cause an out of memory error. Therefore, we cannot use the parallel processing in Featuretools and instead have to build our own implementation with Dask. \n",
    "\n",
    "Fortunately, options such as [Dask](https://dask.pydata.org/en/latest/) make it easy to take advantage of multiple cores on our own machine. In this notebook, we'll see how to run Deep Feature Synthesis in about 3 hours on a personal laptop with 16 GB of RAM. \n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src = \"../../images/dask_logo.png\" width = \"400\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "Following is our plan of action for implementing Dask\n",
    "\n",
    "1. Convert `object` data types to `category`\n",
    "    * This reduces memory consumption significantly\n",
    "2. Create 100 partitions of data and save to disk\n",
    "    * Each partition will contain data from all 7 seven tables for 1/100 of the client ids, `SK_ID_CURR`\n",
    "    * Each partition can be used to make an EntitySet and hence a feature matrix\n",
    "3. Write a function to take a partition and create an `EntitySet`\n",
    "4. Write a function to take an `EntitySet` and calculate a `feature_matrix`\n",
    "    * Since we already have the feature names, we can use `ft.calculate_feature_matrix`\n",
    "5. Use Dask with system processes to generate feature matrices for 8 partitions at a time\n",
    "    * Save these subset feature matrices to disk\n",
    "    * Using proceses will start 8 workers, one for each core, with 2 GB of memory each\n",
    "    * We can't generate the entire feature matrix at once using processes because the final feature matrix is too large to fit on a single core\n",
    "6. Use Dask with threads to read in subset feature matrices and create final feature matrix\n",
    "    * Using threads will start 1 worker with 16 GB of memory, enough to hold the entire feature matrix\n",
    "    * Can save this feature matrix to disk for later use in a machine learning pipeline\n",
    "    \n",
    "This might seem like a lot of tasks, but each one is a relatively simple step. At the end, we'll have a working implementation of Dask that lets us take full advantage of our computing resources. While we could solve this whole problem by just renting a larger machine, this approach will give us a chance to learn about how to work with constraints and engineer a solution. Sometimes having too many resources can limit your creativity, and working with constraints forces us to be innovative! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "# Utilities\n",
    "import sys\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df):\n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory before converting types: 4.38 gb.\n",
      "Total memory after converting types: 2.06 gb.\n"
     ]
    }
   ],
   "source": [
    "# Read in the datasets and replace the anomalous values\n",
    "app_train = pd.read_csv('../input/application_train.csv').replace({365243: np.nan})\n",
    "app_test = pd.read_csv('../input/application_test.csv').replace({365243: np.nan})\n",
    "bureau = pd.read_csv('../input/bureau.csv').replace({365243: np.nan})\n",
    "bureau_balance = pd.read_csv('../input/bureau_balance.csv').replace({365243: np.nan})\n",
    "cash = pd.read_csv('../input/POS_CASH_balance.csv').replace({365243: np.nan})\n",
    "credit = pd.read_csv('../input/credit_card_balance.csv').replace({365243: np.nan})\n",
    "previous = pd.read_csv('../input/previous_application.csv').replace({365243: np.nan})\n",
    "installments = pd.read_csv('../input/installments_payments.csv').replace({365243: np.nan})\n",
    "\n",
    "app_test['TARGET'] = np.nan\n",
    "\n",
    "# Join together training and testing\n",
    "app = app_train.append(app_test, ignore_index = True, sort = True)\n",
    "\n",
    "# Need `SK_ID_CURR` in every dataset\n",
    "bureau_balance = bureau_balance.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], \n",
    "                                      on = 'SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "print(f\"\"\"Total memory before converting types: \\\n",
    "{round(np.sum([x.memory_usage().sum() / 1e9 for x in \n",
    "[app, bureau, bureau_balance, cash, credit, previous, installments]]), 2)} gb.\"\"\")\n",
    "\n",
    "# Convert types to reduce memory usage\n",
    "app = convert_types(app)\n",
    "bureau = convert_types(bureau)\n",
    "bureau_balance = convert_types(bureau_balance)\n",
    "cash = convert_types(cash)\n",
    "credit = convert_types(credit)\n",
    "previous = convert_types(previous)\n",
    "installments = convert_types(installments)\n",
    "\n",
    "print(f\"\"\"Total memory after converting types: \\\n",
    "{round(np.sum([x.memory_usage().sum() / 1e9 for x in \n",
    "[app, bureau, bureau_balance, cash, credit, previous, installments]]), 2)} gb.\"\"\")\n",
    "\n",
    "# Set the index for locating\n",
    "for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "    dataset.set_index('SK_ID_CURR', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object memory usage.\n",
      "0.027462848 gb\n",
      "Category memory usage.\n",
      "0.015448612 gb\n",
      "Length of data:  1716428\n",
      "Number of unique categories:  15\n"
     ]
    }
   ],
   "source": [
    "print('Object memory usage.')\n",
    "print(bureau['CREDIT_TYPE'].astype('object').memory_usage() / 1e9, 'gb')\n",
    "\n",
    "print('Category memory usage.')\n",
    "print(bureau['CREDIT_TYPE'].astype('category').memory_usage() / 1e9, 'gb')\n",
    "\n",
    "print('Length of data: ', bureau.shape[0])\n",
    "print('Number of unique categories: ', bureau['CREDIT_TYPE'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Data\n",
    "\n",
    "Next, we partition the data into 104 separate datasets based on the client id, `SK_ID_CURR`. Each partition by itself can be used to make an `EntitySet` and later a feature matrix. One partition will contain seven data tables, each with only the data associated with the set clients. 104 partitions is sort of an arbitrary number and it might be worth exploring other options to see which works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(user_list, partition):\n",
    "    \"\"\"Creates and saves a dataset with only the users in `user_list`.\"\"\"\n",
    "    \n",
    "    # Make the directory\n",
    "    directory = '../input/partitions/p%d' % (partition + 1)\n",
    "    if os.path.exists(directory):\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        # Subset based on user list\n",
    "        app_subset = app[app.index.isin(user_list)].copy().reset_index()\n",
    "        bureau_subset = bureau[bureau.index.isin(user_list)].copy().reset_index()\n",
    "\n",
    "        # Drop SK_ID_CURR from bureau_balance, cash, credit, and installments\n",
    "        bureau_balance_subset = bureau_balance[bureau_balance.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        cash_subset = cash[cash.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        credit_subset = credit[credit.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        previous_subset = previous[previous.index.isin(user_list)].copy().reset_index()\n",
    "        installments_subset = installments[installments.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        \n",
    "\n",
    "        # Save data to the directory\n",
    "        app_subset.to_csv('%s/app.csv' % directory, index = False)\n",
    "        bureau_subset.to_csv('%s/bureau.csv' % directory, index = False)\n",
    "        bureau_balance_subset.to_csv('%s/bureau_balance.csv' % directory, index = False)\n",
    "        cash_subset.to_csv('%s/cash.csv' % directory, index = False)\n",
    "        credit_subset.to_csv('%s/credit.csv' % directory, index = False)\n",
    "        previous_subset.to_csv('%s/previous.csv' % directory, index = False)\n",
    "        installments_subset.to_csv('%s/installments.csv' % directory, index = False)\n",
    "\n",
    "        if partition % 10 == 0:\n",
    "            print('Saved all files in partition {} to {}.'.format(partition + 1, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break into 104 chunks\n",
    "chunk_size = app.shape[0] // 103\n",
    "\n",
    "# Construct an id list\n",
    "id_list = [list(app.iloc[i:i+chunk_size].index) for i in range(0, app.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids in id_list:         356255.\n",
      "Total length of application data: 356255.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Sanity check that we have not missed any ids\n",
    "print('Number of ids in id_list:         {}.'.format(len(list(chain(*id_list)))))\n",
    "print('Total length of application data: {}.'.format(len(app)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning took 0 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "for i, ids in enumerate(id_list):\n",
    "    # Create a partition based on the ids\n",
    "    create_partition(ids, i)\n",
    "    \n",
    "end = timer()\n",
    "print(f'Partitioning took {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__I already had the partitions made, but running the above cell took 1300 seconds (21 minutes) the first time. __\n",
    "\n",
    "We can independently generate the feature matrix for each partition because the partition contains all the data for that group of clients. These partitioned feature matrices can then be joined together into larger feature matrices, and eventually one single matrix with all of the clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Feature names\n",
    "\n",
    "We already calculated the feature names, so we can read them in. This avoids the need to have to recalculate the features on each partition. Instead of using `ft.dfs`, if we have the feature names, we can use `ft.calculate_feature_matrix` and pass in the `EntitySet` and the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "featurenames = ft.load_features('../input/features.txt')\n",
    "print(len(featurenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature matrix, we'll make 1820 features! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Types\n",
    "\n",
    "If the Automated notebook, we specified the variable types when adding entities to the entityset. However, since we already properly defined the data types for each column, Featuretools will now infer the correct variable type. For example, while before we have Booleans mapped to integers which would be interpreted as numeric, now the Booleans are represented as Booleans and hence will be correctly inferred by Featuretools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_types = {'FLAG_CONT_MOBILE': vtypes.Boolean, 'FLAG_DOCUMENT_10': vtypes.Boolean, 'FLAG_DOCUMENT_11': vtypes.Boolean, 'FLAG_DOCUMENT_12': vtypes.Boolean, 'FLAG_DOCUMENT_13': vtypes.Boolean, 'FLAG_DOCUMENT_14': vtypes.Boolean, 'FLAG_DOCUMENT_15': vtypes.Boolean, 'FLAG_DOCUMENT_16': vtypes.Boolean, 'FLAG_DOCUMENT_17': vtypes.Boolean, 'FLAG_DOCUMENT_18': vtypes.Boolean, 'FLAG_DOCUMENT_19': vtypes.Boolean, 'FLAG_DOCUMENT_2': vtypes.Boolean, 'FLAG_DOCUMENT_20': vtypes.Boolean, 'FLAG_DOCUMENT_21': vtypes.Boolean, 'FLAG_DOCUMENT_3': vtypes.Boolean, 'FLAG_DOCUMENT_4': vtypes.Boolean, 'FLAG_DOCUMENT_5': vtypes.Boolean, 'FLAG_DOCUMENT_6': vtypes.Boolean, 'FLAG_DOCUMENT_7': vtypes.Boolean, 'FLAG_DOCUMENT_8': vtypes.Boolean, 'FLAG_DOCUMENT_9': vtypes.Boolean, 'FLAG_EMAIL': vtypes.Boolean, 'FLAG_EMP_PHONE': vtypes.Boolean, 'FLAG_MOBIL': vtypes.Boolean, 'FLAG_PHONE': vtypes.Boolean, 'FLAG_WORK_PHONE': vtypes.Boolean, 'LIVE_CITY_NOT_WORK_CITY': vtypes.Boolean, 'LIVE_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REG_CITY_NOT_LIVE_CITY': vtypes.Boolean, 'REG_CITY_NOT_WORK_CITY': vtypes.Boolean, 'REG_REGION_NOT_LIVE_REGION': vtypes.Boolean, 'REG_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REGION_RATING_CLIENT': vtypes.Ordinal, 'REGION_RATING_CLIENT_W_CITY': vtypes.Ordinal, 'HOUR_APPR_PROCESS_START': vtypes.Ordinal}\n",
    "# previous_types = {'NFLAG_LAST_APPL_IN_DAY': vtypes.Boolean, \n",
    "#              'NFLAG_INSURED_ON_APPROVAL': vtypes.Boolean}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Create EntitySet from Partition \n",
    "\n",
    "The next function takes a single partition of data and make an `EntitySet`. We won't save these entitysets to disk, but instead will use them in Dask. Therefore, if we want to make any changes to the `EntitySet`, such as adding in interesting values or seed features, we can alter this function and remake the `EntitySet` without having to rewrite all the Entity Sets on disk. Writing the entity sets to disk would be another option if we are sure that they won't ever change. For greater flexibility, we write the data partitions to disk (as done above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path.\"\"\"\n",
    "    \n",
    "    # Read in data\n",
    "    app = pd.read_csv('%s/app.csv' % path)\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % path)\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % path)\n",
    "    previous = pd.read_csv('%s/previous.csv' % path)\n",
    "    credit = pd.read_csv('%s/credit.csv' % path)\n",
    "    installments = pd.read_csv('%s/installments.csv' % path)\n",
    "    cash = pd.read_csv('%s/cash.csv' % path)\n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    # Entities with a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV')\n",
    "\n",
    "    # Entities that do not have a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                  make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                                  make_index = True, index = 'cash_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                                  make_index = True, index = 'installments_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                                  make_index = True, index = 'credit_index')\n",
    "    \n",
    "    # Relationship between app_train and bureau\n",
    "    r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationship between bureau and bureau balance\n",
    "    r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "    # Relationship between current app and previous apps\n",
    "    r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationships between previous apps and cash, installments, and credit\n",
    "    r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "    r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "    r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "    \n",
    "    # Add in the defined relationships\n",
    "    es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                               r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function to make sure it can make an `EntitySet` from a data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: clients\n",
       "  Entities:\n",
       "    app [Rows: 3458, Columns: 122]\n",
       "    bureau [Rows: 16097, Columns: 17]\n",
       "    previous [Rows: 16204, Columns: 37]\n",
       "    bureau_balance [Rows: 166374, Columns: 4]\n",
       "    cash [Rows: 96632, Columns: 8]\n",
       "    installments [Rows: 129130, Columns: 8]\n",
       "    credit [Rows: 35694, Columns: 23]\n",
       "  Relationships:\n",
       "    bureau.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU\n",
       "    previous.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    cash.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    installments.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> previous.SK_ID_PREV"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es1 = entityset_from_partition('../input/partitions/p1')\n",
    "es1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function works as intended. The next step is to write a function that can take a single `EntitySet` and the `features` we want to build, and make a feature matrix. This is simple using `ft.calculate_feature_matrix`. \n",
    "\n",
    "# Function to Create Feature Matrix from EntitySet \n",
    "\n",
    "With the entity set and the feature names, generating the feature matrix is a one-liner in Featuretools. Since we are going to use Dask for parallelizing the operation, we'll set the number of jobs to 1. The `chunk_size` is an extremely important parameter, and I'd suggest experimenting with this to find the optimal value. Since we aren't getting any updates, it might make sense to set the chunk size as large as possible. We can try setting it to all of the instances in a given partition at once using the length of the dataset. This is probably the fastest way to make the feature matrix provided each one can fit entirely in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es, feature_names):\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature names\"\"\"\n",
    "\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_names, \n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = es['app'].df.shape[0])\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we test the function using the entityset from the first partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3458, 1820)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm1 = feature_matrix_from_entityset(es1, featurenames)\n",
    "fm1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have both parts needed to go from a data partition on disk to a feature matrix made using 1/104 of the data. Since we're going to be making 8 feature matrices as once, each one utilizing one core on our system and doing this 13 times, we might expect the process to take around \n",
    "\n",
    "The next step is to get Dask to run this in parallel using all 8 cores on our machine (or however many cores you have available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "We will use the Dask utility `delayed` to parallelize the operation. First, we'll import delayed and set up a `Client` using processes, which will create one worker for each core on the machine. The memory limit of each worker will be the total system memory (16 gb) divided by the number of cores (8). \n",
    "\n",
    "We iterate through each path in a list of the partitions and tell dask to first create the entity set from the partition, then create the featurematrix from the entityset, and append the feature matrix to a list of feature matrices. Once we have the complete list of feature matrices, we can `concat` them all together to get one single feature matrix. \n",
    "\n",
    "However, we can't unfortunately do all the feature matrices in one operation because the final feature matrix is to large to fit on a single core. Therefore, we'll make 13 feature matrices (104 / 8) by making a feature matrix from 8 partitions at once. This is small enough to fit on a single core and can be saved to disk. After making the 13 subset feature matrices and saving them all to disk, we can start a new `Client` using threads. This will create a single worked utilizing all of the system memory. We can use this single worked to `concat`enate the individual feature matrices into a single matrix without exhausting the worker memory. Finally, this feature matrix is saved to disk for modeling.\n",
    "\n",
    "The end result will be a single feature matrix on disk that we can load in and use in any standard machine learning pipeline. We also have the option to use the subset feature matrices in a machine learning pipeline using a method such as [Scikit-Learn's partial_fit](http://scikit-learn.org/stable/modules/scaling_strategies.html) if the classifier in question supports the method (Random Forests do not allow for incremental learning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Use all 8 cores\n",
    "client = Client(processes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:49329': 1,\n",
       " 'tcp://127.0.0.1:49332': 1,\n",
       " 'tcp://127.0.0.1:49334': 1,\n",
       " 'tcp://127.0.0.1:49337': 1,\n",
       " 'tcp://127.0.0.1:49338': 1,\n",
       " 'tcp://127.0.0.1:49339': 1,\n",
       " 'tcp://127.0.0.1:49340': 1,\n",
       " 'tcp://127.0.0.1:49345': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ncores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations of Dask\n",
    "\n",
    "After starting a `Client`, if you have `Bokeh` installed, you can navigate to http://localhost:8787/ to view the status of the workers. Doing this on my machine (8 cores with 16 gb total RAM) gives me:\n",
    "\n",
    "![](../images/process_workers.png)\n",
    "\n",
    "Right now we aren't taxing our system very much! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the tasks complete, you will be able to see more information about the status of Dask and your machine. \n",
    "\n",
    "Next, let's create a list of paths of our partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/partitions/p100',\n",
       " '../input/partitions/p4',\n",
       " '../input/partitions/p3',\n",
       " '../input/partitions/p101',\n",
       " '../input/partitions/p2',\n",
       " '../input/partitions/p5',\n",
       " '../input/partitions/p19',\n",
       " '../input/partitions/p26']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['../input/partitions/%s' % file for file in os.listdir('../input/partitions/') if '.' not in file]\n",
    "paths[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run through 8 partitions at once (because we have 8 cores), for each partition making the Entity Set and the Feature Matrix using one worker. This lets us take full advantage of all the cores on our machine. At any one time, there will be 8 feature matrices under construction. We made the partitions small enough that none of the feature matrices will be too large for the worker. \n",
    "\n",
    "The 8 feature matrices will then be `concat`enated into a single feature matrix that is saved to disk. We repeat this process 13 times (104/ 8) until we have accounted for all of the partitions. Then, we can restart a `Client` using threads and create a single feature matrix (which can fit into all of our system memory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97, 105]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(9, len(paths) + 5, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature matrix 0\n",
      "Feature Matrix 0 complete, Time Elapsed: 572.87 seconds.\n",
      "Starting feature matrix 1\n",
      "Feature Matrix 1 complete, Time Elapsed: 599.75 seconds.\n",
      "Starting feature matrix 2\n",
      "Feature Matrix 2 complete, Time Elapsed: 575.26 seconds.\n",
      "Starting feature matrix 3\n",
      "Feature Matrix 3 complete, Time Elapsed: 550.69 seconds.\n",
      "Starting feature matrix 4\n",
      "Feature Matrix 4 complete, Time Elapsed: 564.22 seconds.\n",
      "Starting feature matrix 5\n",
      "Feature Matrix 5 complete, Time Elapsed: 544.63 seconds.\n",
      "Starting feature matrix 6\n",
      "Feature Matrix 6 complete, Time Elapsed: 488.82 seconds.\n",
      "Starting feature matrix 7\n",
      "Feature Matrix 7 complete, Time Elapsed: 537.68 seconds.\n",
      "Starting feature matrix 8\n",
      "Feature Matrix 8 complete, Time Elapsed: 518.94 seconds.\n",
      "Starting feature matrix 9\n",
      "Feature Matrix 9 complete, Time Elapsed: 532.65 seconds.\n",
      "Starting feature matrix 10\n",
      "Feature Matrix 10 complete, Time Elapsed: 594.18 seconds.\n",
      "Starting feature matrix 11\n",
      "Feature Matrix 11 complete, Time Elapsed: 681.11 seconds.\n",
      "Starting feature matrix 12\n",
      "Feature Matrix 12 complete, Time Elapsed: 570.02 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_index = 0\n",
    "overall_start = timer()\n",
    "\n",
    "# Iterate through 8 paths at a time\n",
    "for i, end_index in enumerate(range(9, len(paths) + 5, 8)):\n",
    "    \n",
    "    # Subset to the 8 paths\n",
    "    if end_index > len(paths):\n",
    "        subset_paths = paths[start_index:]\n",
    "    else:\n",
    "        subset_paths = paths[start_index: end_index]\n",
    "    \n",
    "    # Empty list of feature matrices\n",
    "    fms = []\n",
    "\n",
    "    # Iterate through the paths\n",
    "    for path in subset_paths:\n",
    "\n",
    "        # Make the entityset\n",
    "        es = delayed(entityset_from_partition)(path)\n",
    "\n",
    "        # Make the feature matrix and add to the list\n",
    "        fm = delayed(feature_matrix_from_entityset)(es, feature_names = featurenames)\n",
    "        fms.append(fm)\n",
    "\n",
    "    # Final operation will be to concatenate together all of the feature matrices\n",
    "    X = delayed(pd.concat)(fms, axis = 0)\n",
    "    \n",
    "    print(f\"Starting feature matrix {i}\")\n",
    "    start = timer()\n",
    "    feature_matrix = X.compute()\n",
    "    end = timer()\n",
    "    \n",
    "    print(f\"Feature Matrix {i} complete, Time Elapsed: {round(end - start, 2)} seconds.\")\n",
    "    \n",
    "    # Save the feature matrix to disk\n",
    "    feature_matrix.to_csv('../input/fm/%s.csv' % i, index = True)\n",
    "    \n",
    "    # Start index becomes previous ending index\n",
    "    start_index = end_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have Bokeh installed, you can see quite a bit of system information. For example, here is the Directed Acyclic Graph of the tasks:\n",
    "\n",
    "![](../images/task_graph.png)\n",
    "\n",
    "\n",
    "We can also make sure that we're using all system resources if we take a look at the status:\n",
    "\n",
    "![](../images/status.png)\n",
    "\n",
    "Now our workers are being used. You can take some time to look over the profile to see what operations took the most time.\n",
    "\n",
    "![](../images/profile.png)\n",
    "\n",
    "Now we'll create a list of feature matrix paths. There should be a total of 13 stored feature matrices on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for feature matrices\n",
    "base = '../input/fm/'\n",
    "fm_paths = [base + p for p in os.listdir(base) if '.csv' in p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For joining together all of these matrices, we can start a new `Client` and use `delayed` to read in all the matrices and append them to a list. These can then be concatenated together into a final feature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inproc://192.168.1.113/58123/2': 8}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a new client with processes\n",
    "client = Client(processes = False)\n",
    "\n",
    "client.ncores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying `processes=False`, we tell Dask to use threads. This has the result of creating one worker will all of the cores on our machine. Therefore, we don't have to worry about exceeding the memory of a worker on just one core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/distributed/_concurrent_futures_thread.py:64: DtypeWarning: Columns (385,1476) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  result = self.fn(*self.args, **self.kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elasped: 126.1 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Empty list for feature matrices\n",
    "fms = []\n",
    "\n",
    "# Iterate through the feature matrices\n",
    "for path in fm_paths:\n",
    "    # Read in each dataframe and append to list\n",
    "    X = delayed(pd.read_csv)(path, index_col = 0)\n",
    "    fms.append(X)\n",
    "\n",
    "# Concatenate all the matrices together (append rows)\n",
    "fm_out = delayed(pd.concat)(fms, axis = 0)\n",
    "\n",
    "# Time how long operate takes\n",
    "start = timer()\n",
    "feature_matrix = fm_out.compute()\n",
    "end = timer()\n",
    "\n",
    "print(f'Time elasped: {round(end - start, 2)} seconds.')\n",
    "overall_end = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352797, 1820)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature matrix is exactly the expected shape: the number of clients in `app` by the number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time for Feature Matrix Calculation: 8135.78 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Time for Feature Matrix Calculation: {round(overall_end - overall_start, 2)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Working with constraints, such as limited computing power, leads to innovation. In this notebook, we had to engineer a solution to calculating the feature matrix in Dask to complete the task in a reasonable amount of time on a personal machine. Our approach was as follows:\n",
    "\n",
    "1. Partition the data into sets based on the clients\n",
    "2. Write a function to generate an `EntitySet` from a partition\n",
    "3. Write a function to create a `feature_matrix` from an `EntitySet`\n",
    "4. Set up Dask to use all 8 cores to make a feature matrix from 8 partitions at once\n",
    "5. Add the individual feature matrices together to create on feature matrix with all clients\n",
    "6. Save the resulting feature matrix for a machine learning pipeline\n",
    "\n",
    "Parallel processing allows us to take full advantage of our system's resources. Thanks to libraries such as Dask, we can run operations in parallel and reduce computation time by 10x or more. The same framework developed in this notebook can be applied to other data science and machine learning problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
