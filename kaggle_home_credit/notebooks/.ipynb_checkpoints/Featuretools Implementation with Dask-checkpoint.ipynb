{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools Implementation with Dask\n",
    "\n",
    "In this notebook we will use Dask to run deep feature synthesis on the entire dataset and generate the feature matrix. This operation is not feasible with a personal laptop on the entire Kaggle Home Credit dataset, but using Dask we can run the operations in parallel and complete this operation on a laptop in a reasonable time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Feature names\n",
    "\n",
    "We already calculated the feature names, so it's simple to read them in. This avoids the need to have to recalculate the features on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "featurenames = ft.load_features('../input/feature_names.txt')\n",
    "print(len(featurenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Types\n",
    "\n",
    "Following are the variable types defined in the Automated Feature Engineering notebook. Defining them here prevents us from needing to define them for every entity separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_types = {'FLAG_CONT_MOBILE': vtypes.Boolean, 'FLAG_DOCUMENT_10': vtypes.Boolean, 'FLAG_DOCUMENT_11': vtypes.Boolean, 'FLAG_DOCUMENT_12': vtypes.Boolean, 'FLAG_DOCUMENT_13': vtypes.Boolean, 'FLAG_DOCUMENT_14': vtypes.Boolean, 'FLAG_DOCUMENT_15': vtypes.Boolean, 'FLAG_DOCUMENT_16': vtypes.Boolean, 'FLAG_DOCUMENT_17': vtypes.Boolean, 'FLAG_DOCUMENT_18': vtypes.Boolean, 'FLAG_DOCUMENT_19': vtypes.Boolean, 'FLAG_DOCUMENT_2': vtypes.Boolean, 'FLAG_DOCUMENT_20': vtypes.Boolean, 'FLAG_DOCUMENT_21': vtypes.Boolean, 'FLAG_DOCUMENT_3': vtypes.Boolean, 'FLAG_DOCUMENT_4': vtypes.Boolean, 'FLAG_DOCUMENT_5': vtypes.Boolean, 'FLAG_DOCUMENT_6': vtypes.Boolean, 'FLAG_DOCUMENT_7': vtypes.Boolean, 'FLAG_DOCUMENT_8': vtypes.Boolean, 'FLAG_DOCUMENT_9': vtypes.Boolean, 'FLAG_EMAIL': vtypes.Boolean, 'FLAG_EMP_PHONE': vtypes.Boolean, 'FLAG_MOBIL': vtypes.Boolean, 'FLAG_PHONE': vtypes.Boolean, 'FLAG_WORK_PHONE': vtypes.Boolean, 'LIVE_CITY_NOT_WORK_CITY': vtypes.Boolean, 'LIVE_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REG_CITY_NOT_LIVE_CITY': vtypes.Boolean, 'REG_CITY_NOT_WORK_CITY': vtypes.Boolean, 'REG_REGION_NOT_LIVE_REGION': vtypes.Boolean, 'REG_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REGION_RATING_CLIENT': vtypes.Ordinal, 'REGION_RATING_CLIENT_W_CITY': vtypes.Ordinal, 'HOUR_APPR_PROCESS_START': vtypes.Ordinal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_types = {'NFLAG_LAST_APPL_IN_DAY': vtypes.Boolean, \n",
    "             'NFLAG_INSURED_ON_APPROVAL': vtypes.Boolean}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Create EntitySet from Partition \n",
    "\n",
    "The data has aleady been broken into 50 partitions so we will write a function that takes a single partition and creates the entity set for that data. This can then be passed into a function that calculates the feature matrix from the entityset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path\"\"\"\n",
    "    \n",
    "    app = pd.read_csv('%s/app.csv' % path)\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % path)\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % path)\n",
    "    previous = pd.read_csv('%s/previous.csv' % path)\n",
    "    credit = pd.read_csv('%s/credit.csv' % path)\n",
    "    installments = pd.read_csv('%s/installments.csv' % path)\n",
    "    cash = pd.read_csv('%s/cash.csv' % path)\n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    # Entities with a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR',\n",
    "                                  variable_types = app_types)\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV',\n",
    "                                  variable_types = previous_types)\n",
    "\n",
    "    # Entities that do not have a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                  make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                                  make_index = True, index = 'cash_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                                  make_index = True, index = 'installments_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                                  make_index = True, index = 'credit_index')\n",
    "    \n",
    "    # Relationship between app_train and bureau\n",
    "    r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationship between bureau and bureau balance\n",
    "    r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "    # Relationship between current app and previous apps\n",
    "    r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationships between previous apps and cash, installments, and credit\n",
    "    r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "    r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "    r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "    \n",
    "    # Add in the defined relationships\n",
    "    es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                               r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function to make sure it can make an `EntitySet` from a data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: clients\n",
       "  Entities:\n",
       "    app [Rows: 7125, Columns: 122]\n",
       "    bureau [Rows: 33478, Columns: 17]\n",
       "    previous [Rows: 32795, Columns: 37]\n",
       "    bureau_balance [Rows: 346658, Columns: 4]\n",
       "    cash [Rows: 197046, Columns: 8]\n",
       "    installments [Rows: 264809, Columns: 8]\n",
       "    credit [Rows: 74219, Columns: 23]\n",
       "  Relationships:\n",
       "    bureau.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU\n",
       "    previous.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    cash.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    installments.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> previous.SK_ID_PREV"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es1 = entityset_from_partition('../input/partitions/p1')\n",
    "es1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works! The next step is to create a feature matrix for the partition from the `EntitySet` and the `features`. \n",
    "\n",
    "## Function to Create Featurematrix from EntitySet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es, feature_names):\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature names\"\"\"\n",
    "\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_names, \n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = 1000)\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to workers in 10.319 seconds\n",
      "\n",
      "Elapsed: 00:00 | Remaining: ? | Progress:   0%|          | Calculated: 0/8 chunks\u001b[A\n",
      "Elapsed: 03:53 | Remaining: 27:17 | Progress:  12%|█▎        | Calculated: 1/8 chunks\u001b[A\n",
      "Elapsed: 05:14 | Remaining: 18:46 | Progress:  25%|██▌       | Calculated: 2/8 chunks\u001b[A\n",
      "Elapsed: 05:14 | Remaining: 10:58 | Progress:  38%|███▊      | Calculated: 3/8 chunks\u001b[A\n",
      "Elapsed: 05:16 | Remaining: 06:10 | Progress:  50%|█████     | Calculated: 4/8 chunks\u001b[A\n",
      "Elapsed: 05:16 | Remaining: 02:09 | Progress:  75%|███████▌  | Calculated: 6/8 chunks\u001b[A\n",
      "Elapsed: 05:17 | Remaining: 00:45 | Progress:  88%|████████▊ | Calculated: 7/8 chunks\u001b[A\n",
      "Elapsed: 05:17 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 8/8 chunks\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7125, 1820)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm1 = feature_matrix_from_entityset(es1, featurenames)\n",
    "fm1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the parts needed to create our feature matrixes. The last step is to get Dask to run this in parallel. For the dask implementation, we'll set `n_jobs = 1` and `verbose = 0` because Dask already runs the calculation in parallel and because Dask should have a progress bar that we can view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "We will use the Dask utility `delayed` to parallelize the operation. We iterate through each path in a list of the partitions and tell dask to first create the entity set from the partition, then create the featurematrix from the entityset, and finally, append the feature matrix to a list of feature matrices. The last step is to `concat`enate all of the feature matrices together to get one final matrix that we save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/partitions/p4',\n",
       " '../input/partitions/p3',\n",
       " '../input/partitions/p2',\n",
       " '../input/partitions/p5',\n",
       " '../input/partitions/p19',\n",
       " '../input/partitions/p26',\n",
       " '../input/partitions/p21',\n",
       " '../input/partitions/p28']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['../input/partitions/%s' % file for file in os.listdir('../input/partitions/')]\n",
    "paths[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('concat-4fe40087-a5b1-4847-8f5e-b4026d2b2554')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty list of feature matrices\n",
    "fms = []\n",
    "\n",
    "# Iterate through the paths\n",
    "for path in paths:\n",
    "    if '.DS_Store' in path:\n",
    "        next\n",
    "    else:\n",
    "        # Make the entityset\n",
    "        es = delayed(entityset_from_partition)(path)\n",
    "\n",
    "        # Make the feature matrix\n",
    "        fm = delayed(feature_matrix_from_entityset)(es, feature_names = featurenames)\n",
    "        fms.append(fm)\n",
    "    \n",
    "# Final operation will be to concatenate together all of the feature matrices\n",
    "X = delayed(pd.concat)(fms, axis = 0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[######                                  ] | 15% Completed |  3hr 23min  9.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/featuretools/computational_backends/utils.py:102: UserWarning: Chunk size is greater than size of feature matrix\n",
      "  warnings.warn(\"Chunk size is greater than size of feature matrix\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[######################################  ] | 95% Completed | 20hr 46min 14.2s"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "# Context for progress and profiling\n",
    "with ProgressBar(), Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    feature_matrix = X.compute()\n",
    "\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature matrix\n",
    "feature_matrix.to_csv('../input/feature_matrix.csv', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations of Run\n",
    "\n",
    "We can use Bokeh and the built in plotting capabilities of Dask to plot the resources used during the computation. This is not necessary for the project but might be interesting for understanding how Dask operates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprof.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprof.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we used Dask to complete an operation that normally would have taken far more resources than we have available on our personal computer. Although featuretools supports parallel processing, there are still some issues that need to be worked out, and we can take matters into our own hands to get the job done! This will allow anyone with reasonable hardware to take advantage of featuretools! The next notebook is Sampling and Feature Selection where we limit reduce the numbers of features to allow for reasonable modeling times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
