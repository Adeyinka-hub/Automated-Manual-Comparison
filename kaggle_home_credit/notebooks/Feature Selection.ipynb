{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "The purpose of this notebook is perform feature selection. First we will have to sample the data to 10% of the original observations. This is necessary because the large size of the full feature matrices does not allow for efficient feature selection or for a significant number of random search iterations.\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "1. Sample 10% of the training observations randomly\n",
    "2. Convert numeric columns to `np.float32`\n",
    "3. Convert boolean columns to `np.uint8`\n",
    "4. One-hot encode categorical features as necessary\n",
    "5. Remove one of every pair of columns with all duplicated values (1.0 correlation)\n",
    "6. Remove columns with more than 90% missing values\n",
    "7. Remove columns with a single unique value\n",
    "8. Remove one of every pair of columns with abs(correlation) > 0.95\n",
    "\n",
    "The reduced data set will then be saved as `_sample.csv` and can be used for 100 iterations of random search with the Gradient Boosting Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = pd.read_csv('../input/feature_matrix.csv', low_memory=False)\n",
    "\n",
    "# Sampling 10% of the original data\n",
    "train = feature_matrix[feature_matrix['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "del feature_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE) not in data\n",
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Closed) not in data\n",
      "SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Active) not in data\n",
      "SUM(bureau_balance.bureau.PREVIOUS_OTHER_LOAN_RATE) not in data\n"
     ]
    }
   ],
   "source": [
    "for col in ['SUM(bureau.PREVIOUS_OTHER_LOAN_RATE)', 'SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Closed)',\n",
    "            'SUM(bureau.PREVIOUS_OTHER_LOAN_RATE WHERE CREDIT_ACTIVE = Active)', 'SUM(bureau_balance.bureau.PREVIOUS_OTHER_LOAN_RATE)']:\n",
    "    try:\n",
    "        train[col] = train[col].astype(np.float32)\n",
    "    except:\n",
    "        print(f'{col} not in data')\n",
    "    \n",
    "for col in train:\n",
    "    if train[col].dtype == 'bool':\n",
    "        train[col] = train[col].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 2091)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.get_dummies(train)\n",
    "n_features_start = train.shape[1] - 2\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns with duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1816)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, idx, inv, counts = np.unique(train, axis = 1, return_index = True, return_inverse=True, return_counts=True)\n",
    "train = train.iloc[:, idx]\n",
    "n_non_unique_columns = n_features_start - train.shape[1] - 2\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "The threshold is currently set at 90% but could be lowered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1798)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_threshold = 90\n",
    "\n",
    "# Find missing and percentage\n",
    "missing = pd.DataFrame(train.isnull().sum())\n",
    "missing['percent'] = 100 * (missing[0] / train.shape[0])\n",
    "missing.sort_values('percent', ascending = False, inplace = True)\n",
    "\n",
    "# Missing above threshold\n",
    "missing_cols = list(missing[missing['percent'] > missing_threshold].index)\n",
    "n_missing_cols = len(missing_cols)\n",
    "\n",
    "train = train[[x for x in train if x not in missing_cols]]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero variance columns\n",
    "\n",
    "These are any columns with only a single unique value. (`np.nan` does not count as a unique value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30751, 1771)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts = pd.DataFrame(train.nunique()).sort_values(0, ascending = True)\n",
    "zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "train = train[[x for x in train if x not in zero_variance_cols]]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns containing derivations of target\n",
    "\n",
    "This is a special consideration we have to take with the Featuretools data because one of the columns is derived from the `TARGET`. (PERCENTILE transformation works on numeric columns of the data and I accidentally left in the `TARGET` when running the feature synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n",
      "PERCENTILE(TARGET)\n"
     ]
    }
   ],
   "source": [
    "for col in train:\n",
    "    if 'TARGET' in col:\n",
    "        print(col)\n",
    "        \n",
    "train.drop(columns = 'PERCENTILE(TARGET)', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Collinear Variables with Correlation Threshold\n",
    "\n",
    "The correlation threshold will be set at 0.95 and one out of every pair of columns that are above this threshold will be removed. The column removed is the one that occurs last in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.95\n",
    "\n",
    "corr_matrix = train.corr()\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Select the features with correlations above the threshold\n",
    "# Need to use the absolute value\n",
    "to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[x for x in train if x not in to_drop]]\n",
    "n_collinear = len(to_drop)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_non_unique_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_zero_variance_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_collinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_removed = n_non_unique_columns + n_missing_cols + n_zero_variance_cols + n_collinear + 1\n",
    "print('Total columns removed: ', total_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save sample of Data\n",
    "\n",
    "The resulting data has 10% of the training observations and 1042 columns (1040 of which are features). This data is now ready for random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../input/feature_matrix_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Feature Selection\n",
    "\n",
    "We can refactor the four steps completed above into a single function that applies them in the same sequence to any dataframe. This function will be used on the manual and semi-automated feaures with the same inputs as with the Featuretools features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(feature_matrix, missing_threshold=90, correlation_threshold=0.95):\n",
    "    \"\"\"Feature selection for a dataframe.\"\"\"\n",
    "    \n",
    "    feature_matrix = pd.get_dummies(feature_matrix)\n",
    "    n_features_start = feature_matrix.shape[1]\n",
    "    print('Original shape: ', feature_matrix.shape)\n",
    "\n",
    "    _, idx = np.unique(feature_matrix, axis = 1, return_index = True)\n",
    "    feature_matrix = feature_matrix.iloc[:, idx]\n",
    "    n_non_unique_columns = n_features_start - feature_matrix.shape[1]\n",
    "    print('{}  non-unique valued columns.'.format(n_non_unique_columns))\n",
    "\n",
    "    # Find missing and percentage\n",
    "    missing = pd.DataFrame(feature_matrix.isnull().sum())\n",
    "    missing['percent'] = 100 * (missing[0] / feature_matrix.shape[0])\n",
    "    missing.sort_values('percent', ascending = False, inplace = True)\n",
    "\n",
    "    # Missing above threshold\n",
    "    missing_cols = list(missing[missing['percent'] > missing_threshold].index)\n",
    "    n_missing_cols = len(missing_cols)\n",
    "\n",
    "    # Remove missing columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\n",
    "    print('{} missing columns with threshold: {}.'.format(n_missing_cols,\n",
    "                                                                        missing_threshold))\n",
    "    \n",
    "    # Zero variance\n",
    "    unique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\n",
    "    zero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\n",
    "    n_zero_variance_cols = len(zero_variance_cols)\n",
    "\n",
    "    # Remove zero variance columns\n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\n",
    "    print('{} zero variance columns.'.format(n_zero_variance_cols))\n",
    "    \n",
    "    # Correlations\n",
    "    corr_matrix = feature_matrix.corr()\n",
    "\n",
    "    # Extract the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "    # Select the features with correlations above the threshold\n",
    "    # Need to use the absolute value\n",
    "    to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "    n_collinear = len(to_drop)\n",
    "    \n",
    "    feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\n",
    "    print('{} collinear columns removed with threshold: {}.'.format(n_collinear,\n",
    "                                                                          correlation_threshold))\n",
    "    \n",
    "    total_removed = n_non_unique_columns + n_missing_cols + n_zero_variance_cols + n_collinear\n",
    "    \n",
    "    print('Total columns removed: ', total_removed)\n",
    "    print('Shape after feature selection: {}.'.format(feature_matrix.shape))\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Features\n",
    "\n",
    "The process is the same except now we have a function to carry out the feature selection. First we subset to 10% of the training data, and then we apply feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features = pd.read_csv('../input/features_manual.csv')\n",
    "manual_features = manual_features[manual_features['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "manual_features = feature_selection(manual_features, 90, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can save the features for random search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features.to_csv('../input/features_manual_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Automated Features\n",
    "\n",
    "The final feature matrix is that created by what we called __semi-automated__ feature engineering. The same exact process applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features = pd.read_csv('../input/features_semi.csv')\n",
    "semi_features = semi_features[semi_features['TARGET'].notnull()].sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "semi_features = feature_selection(semi_features, 90, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features.to_csv('../input/features_semi_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Features\n",
    "\n",
    "These are the features available in the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('../input/application_train.csv')\n",
    "fm = fm.sample(frac = 0.1, random_state = 50)\n",
    "\n",
    "fm = pd.get_dummies(fm)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = feature_selection(fm, 90, 0.95)\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.to_csv('../input/features_default_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions from Feature Selection\n",
    "\n",
    "In order to allow us to perform feature selection, we first had to limit the number of rows of data to 10% of the training observations. Then we were able to apply feature selection to reduce the data dimensionality. The final sampled and selected data was saved as `_sample.csv` and can now be used for random search with the Gradient Boosting Machine. There are a number of \"arbitrary\" thresholds in this approach, but by applying the same operations to all 3 datasets, it is hoped that these choices will not affect the integrity of the analysis. At the end of the day, some results are better than no results and these operations will allow us to perform 100 iterations of random search and proceed with the project. \n",
    "\n",
    "The next step is Subsetting Data where we use these results in order to create versions of the feature matrices with all the observations but only the columns identified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting Data\n",
    "\n",
    "Now that we have identified the features we want to keep, we can subset the full data matrices to these features. The `_sample.csv` files are the sampled version (10% of the observations) with the feature selection applied. The `fm` dataframes are the full data. The full data after being subsetted is then saved as `_selected.csv`.\n",
    "\n",
    "The random search is run on the `_sample.csv` files, but the `_selected.csv` files will be used to train and test the final model. These files have all the observations, but only the columns that remained after feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = pd.read_csv('../input/application_train.csv'), pd.read_csv('../input/application_test.csv')\n",
    "test['TARGET'] = np.nan\n",
    "train, test = pd.get_dummies(train).align(pd.get_dummies(test), axis = 1, join = 'inner')\n",
    "\n",
    "sample = pd.read_csv('../input/features_default_sample.csv')\n",
    "fm = train.append(test, ignore_index = True)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = fm[[x for x in sample.columns if x in fm.columns]]\n",
    "fm.to_csv('../input/features_default_selected.csv')\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/features_manual_sample.csv')\n",
    "fm = pd.read_csv('../input/features_manual.csv')\n",
    "\n",
    "# One-hot encoding\n",
    "fm = pd.get_dummies(fm)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to the columns in the sample\n",
    "fm = fm[sample.columns]\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Engineering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sample and full data\n",
    "sample = pd.read_csv('../input/feature_matrix_sample.csv')\n",
    "fm = pd.read_csv('../input/feature_matrix.csv')\n",
    "\n",
    "print(fm.shape)\n",
    "\n",
    "# One hot encoding\n",
    "cat = pd.get_dummies(fm.select_dtypes('object'))\n",
    "\n",
    "# Convert the column types\n",
    "for col in fm:\n",
    "    if fm[col].dtype == 'bool':\n",
    "        fm[col] = fm[col].astype(np.uint8)\n",
    "        \n",
    "# Add the one-hot encoded columns\n",
    "fm = fm.select_dtypes(['number'])\n",
    "fm = pd.concat([fm, cat], axis = 1)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/feature_matrix_sample.csv')\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to the columns in the sample\n",
    "fm = fm[[x for x in sample.columns if x in fm]]\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "fm.to_csv('../input/feature_matrix_selected.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Automated Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/features_semi_sample.csv')\n",
    "fm = pd.read_csv('../input/features_semi.csv')\n",
    "\n",
    "# One hot encoding\n",
    "fm = pd.get_dummies(fm)\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to the columns in sample\n",
    "fm = fm[sample.columns]\n",
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.to_csv('../input/features_semi_selected.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The resulting data can now be used for modeling. The columns have been reduced through feature selection but all of the observations remain. The final datasets will be tested both with the default gradient boosting machine hyperparameters and with the optimal hyperparameters found from random search. \n",
    "\n",
    "The next step is to run random search on these results. This is implemented in the `random_search.py` script in the scripts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
