{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools Implementation with Dask\n",
    "\n",
    "In this notebook we will use Dask to run deep feature synthesis on the entire dataset and generate the feature matrix. This operation is not feasible with a personal laptop on the entire Kaggle Home Credit dataset at once, but using Dask we can run the operations in parallel and complete this operation on a laptop in a reasonable time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Data\n",
    "\n",
    "First, we partition the data into 100 separate datasets based on the client id, `SK_ID_CURR`. These partitions can then be used in combination with Dask to take advantage of all the resources on our machine for generating the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets and replace the anomalous values\n",
    "app_train = pd.read_csv('../input/application_train.csv').replace({365243: np.nan})\n",
    "app_test = pd.read_csv('../input/application_test.csv').replace({365243: np.nan})\n",
    "bureau = pd.read_csv('../input/bureau.csv').replace({365243: np.nan})\n",
    "bureau_balance = pd.read_csv('../input/bureau_balance.csv').replace({365243: np.nan})\n",
    "cash = pd.read_csv('../input/POS_CASH_balance.csv').replace({365243: np.nan})\n",
    "credit = pd.read_csv('../input/credit_card_balance.csv').replace({365243: np.nan})\n",
    "previous = pd.read_csv('../input/previous_application.csv').replace({365243: np.nan})\n",
    "installments = pd.read_csv('../input/installments_payments.csv').replace({365243: np.nan})\n",
    "\n",
    "app_test['TARGET'] = np.nan\n",
    "\n",
    "# Join together training and testing\n",
    "app = app_train.append(app_test, ignore_index = True, sort = True)\n",
    "\n",
    "# All ids should be integers\n",
    "for index in ['SK_ID_CURR', 'SK_ID_PREV', 'SK_ID_BUREAU']:\n",
    "    for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "        if index in list(dataset.columns):\n",
    "            # Convert to integers after filling in missing values (not sure why values are missing)\n",
    "            dataset[index] = dataset[index].fillna(0).astype(np.int64)\n",
    "\n",
    "# Need `SK_ID_CURR` in every dataset\n",
    "bureau_balance = bureau_balance.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], \n",
    "                                      on = 'SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "\n",
    "# Set the index for locating\n",
    "for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "    dataset.set_index('SK_ID_CURR', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(user_list, partition):\n",
    "    \"\"\"Creates an entityset with only the users in `user_list`. \n",
    "       Main purpose is partioning data\"\"\"\n",
    "    \n",
    "    # Subset based on user list\n",
    "    app_subset = app[app.index.isin(user_list)].copy().reset_index()\n",
    "    bureau_subset = bureau[bureau.index.isin(user_list)].copy().reset_index()\n",
    "    \n",
    "    # Drop SK_ID_CURR from bureau_balance, cash, credit, and installments\n",
    "    bureau_balance_subset = bureau_balance[bureau_balance.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "    cash_subset = cash[cash.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "    credit_subset = credit[credit.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "    previous_subset = previous[previous.index.isin(user_list)].copy().reset_index()\n",
    "    installments_subset = installments[installments.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "    \n",
    "    # Make the directory\n",
    "    directory = '../input/partitions/p%d' % (partition + 1)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Save data to the directory\n",
    "    app_subset.to_csv('%s/app.csv' % directory, index = False)\n",
    "    bureau_subset.to_csv('%s/bureau.csv' % directory, index = False)\n",
    "    bureau_balance_subset.to_csv('%s/bureau_balance.csv' % directory, index = False)\n",
    "    cash_subset.to_csv('%s/cash.csv' % directory, index = False)\n",
    "    credit_subset.to_csv('%s/credit.csv' % directory, index = False)\n",
    "    previous_subset.to_csv('%s/previous.csv' % directory, index = False)\n",
    "    installments_subset.to_csv('%s/installments.csv' % directory, index = False)\n",
    "\n",
    "    print('Saved all files in partition {} to {}.'.format(partition + 1, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break into 100 chunks\n",
    "chunk_size = app.shape[0] // 100\n",
    "\n",
    "# Construct an id list\n",
    "id_list = [list(app.iloc[i:i+chunk_size].index) for i in range(0, app.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids in id_list:         356255.\n",
      "Total length of application data: 356255.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Sanity check that we have not missed any ids\n",
    "print('Number of ids in id_list:         {}.'.format(len(list(chain(*id_list)))))\n",
    "print('Total length of application data: {}.'.format(len(app)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all files in partition 1 to ../input/partitions/p1.\n",
      "Saved all files in partition 2 to ../input/partitions/p2.\n",
      "Saved all files in partition 3 to ../input/partitions/p3.\n",
      "Saved all files in partition 4 to ../input/partitions/p4.\n",
      "Saved all files in partition 5 to ../input/partitions/p5.\n",
      "Saved all files in partition 6 to ../input/partitions/p6.\n",
      "Saved all files in partition 7 to ../input/partitions/p7.\n",
      "Saved all files in partition 8 to ../input/partitions/p8.\n",
      "Saved all files in partition 9 to ../input/partitions/p9.\n",
      "Saved all files in partition 10 to ../input/partitions/p10.\n",
      "Saved all files in partition 11 to ../input/partitions/p11.\n",
      "Saved all files in partition 12 to ../input/partitions/p12.\n",
      "Saved all files in partition 13 to ../input/partitions/p13.\n",
      "Saved all files in partition 14 to ../input/partitions/p14.\n",
      "Saved all files in partition 15 to ../input/partitions/p15.\n",
      "Saved all files in partition 16 to ../input/partitions/p16.\n",
      "Saved all files in partition 17 to ../input/partitions/p17.\n",
      "Saved all files in partition 18 to ../input/partitions/p18.\n",
      "Saved all files in partition 19 to ../input/partitions/p19.\n",
      "Saved all files in partition 20 to ../input/partitions/p20.\n",
      "Saved all files in partition 21 to ../input/partitions/p21.\n",
      "Saved all files in partition 22 to ../input/partitions/p22.\n",
      "Saved all files in partition 23 to ../input/partitions/p23.\n",
      "Saved all files in partition 24 to ../input/partitions/p24.\n",
      "Saved all files in partition 25 to ../input/partitions/p25.\n",
      "Saved all files in partition 26 to ../input/partitions/p26.\n",
      "Saved all files in partition 27 to ../input/partitions/p27.\n",
      "Saved all files in partition 28 to ../input/partitions/p28.\n",
      "Saved all files in partition 29 to ../input/partitions/p29.\n",
      "Saved all files in partition 30 to ../input/partitions/p30.\n",
      "Saved all files in partition 31 to ../input/partitions/p31.\n",
      "Saved all files in partition 32 to ../input/partitions/p32.\n",
      "Saved all files in partition 33 to ../input/partitions/p33.\n",
      "Saved all files in partition 34 to ../input/partitions/p34.\n",
      "Saved all files in partition 35 to ../input/partitions/p35.\n",
      "Saved all files in partition 36 to ../input/partitions/p36.\n",
      "Saved all files in partition 37 to ../input/partitions/p37.\n",
      "Saved all files in partition 38 to ../input/partitions/p38.\n",
      "Saved all files in partition 39 to ../input/partitions/p39.\n",
      "Saved all files in partition 40 to ../input/partitions/p40.\n",
      "Saved all files in partition 41 to ../input/partitions/p41.\n",
      "Saved all files in partition 42 to ../input/partitions/p42.\n",
      "Saved all files in partition 43 to ../input/partitions/p43.\n",
      "Saved all files in partition 44 to ../input/partitions/p44.\n",
      "Saved all files in partition 45 to ../input/partitions/p45.\n",
      "Saved all files in partition 46 to ../input/partitions/p46.\n",
      "Saved all files in partition 47 to ../input/partitions/p47.\n",
      "Saved all files in partition 48 to ../input/partitions/p48.\n",
      "Saved all files in partition 49 to ../input/partitions/p49.\n",
      "Saved all files in partition 50 to ../input/partitions/p50.\n",
      "Saved all files in partition 51 to ../input/partitions/p51.\n",
      "Saved all files in partition 52 to ../input/partitions/p52.\n",
      "Saved all files in partition 53 to ../input/partitions/p53.\n",
      "Saved all files in partition 54 to ../input/partitions/p54.\n",
      "Saved all files in partition 55 to ../input/partitions/p55.\n",
      "Saved all files in partition 56 to ../input/partitions/p56.\n",
      "Saved all files in partition 57 to ../input/partitions/p57.\n",
      "Saved all files in partition 58 to ../input/partitions/p58.\n",
      "Saved all files in partition 59 to ../input/partitions/p59.\n",
      "Saved all files in partition 60 to ../input/partitions/p60.\n",
      "Saved all files in partition 61 to ../input/partitions/p61.\n",
      "Saved all files in partition 62 to ../input/partitions/p62.\n",
      "Saved all files in partition 63 to ../input/partitions/p63.\n",
      "Saved all files in partition 64 to ../input/partitions/p64.\n",
      "Saved all files in partition 65 to ../input/partitions/p65.\n",
      "Saved all files in partition 66 to ../input/partitions/p66.\n",
      "Saved all files in partition 67 to ../input/partitions/p67.\n",
      "Saved all files in partition 68 to ../input/partitions/p68.\n",
      "Saved all files in partition 69 to ../input/partitions/p69.\n",
      "Saved all files in partition 70 to ../input/partitions/p70.\n",
      "Saved all files in partition 71 to ../input/partitions/p71.\n",
      "Saved all files in partition 72 to ../input/partitions/p72.\n",
      "Saved all files in partition 73 to ../input/partitions/p73.\n",
      "Saved all files in partition 74 to ../input/partitions/p74.\n",
      "Saved all files in partition 75 to ../input/partitions/p75.\n",
      "Saved all files in partition 76 to ../input/partitions/p76.\n",
      "Saved all files in partition 77 to ../input/partitions/p77.\n",
      "Saved all files in partition 78 to ../input/partitions/p78.\n",
      "Saved all files in partition 79 to ../input/partitions/p79.\n",
      "Saved all files in partition 80 to ../input/partitions/p80.\n",
      "Saved all files in partition 81 to ../input/partitions/p81.\n",
      "Saved all files in partition 82 to ../input/partitions/p82.\n",
      "Saved all files in partition 83 to ../input/partitions/p83.\n",
      "Saved all files in partition 84 to ../input/partitions/p84.\n",
      "Saved all files in partition 85 to ../input/partitions/p85.\n",
      "Saved all files in partition 86 to ../input/partitions/p86.\n",
      "Saved all files in partition 87 to ../input/partitions/p87.\n",
      "Saved all files in partition 88 to ../input/partitions/p88.\n",
      "Saved all files in partition 89 to ../input/partitions/p89.\n",
      "Saved all files in partition 90 to ../input/partitions/p90.\n",
      "Saved all files in partition 91 to ../input/partitions/p91.\n",
      "Saved all files in partition 92 to ../input/partitions/p92.\n",
      "Saved all files in partition 93 to ../input/partitions/p93.\n",
      "Saved all files in partition 94 to ../input/partitions/p94.\n",
      "Saved all files in partition 95 to ../input/partitions/p95.\n",
      "Saved all files in partition 96 to ../input/partitions/p96.\n",
      "Saved all files in partition 97 to ../input/partitions/p97.\n",
      "Saved all files in partition 98 to ../input/partitions/p98.\n",
      "Saved all files in partition 99 to ../input/partitions/p99.\n",
      "Saved all files in partition 100 to ../input/partitions/p100.\n",
      "Saved all files in partition 101 to ../input/partitions/p101.\n"
     ]
    }
   ],
   "source": [
    "for i, ids in enumerate(id_list):\n",
    "    # Create a partition based on the ids\n",
    "    create_partition(ids, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from Partitioning Data\n",
    "\n",
    "Now that the data has been partitioned into 100 sections, we can use Dask to parallize computing the feature matrix. We can independently generate the feature matrix for each partition because the partition contains all the data for that group of clients. These partitioned feature matrices can then be joined together into one large feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Feature names\n",
    "\n",
    "We already calculated the feature names, so it's simple to read them in. This avoids the need to have to recalculate the features on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "featurenames = ft.load_features('../input/feature_names.txt')\n",
    "print(len(featurenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Types\n",
    "\n",
    "Following are the variable types defined in the Automated Feature Engineering notebook. Defining them here prevents us from needing to define them for every entity separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_types = {'FLAG_CONT_MOBILE': vtypes.Boolean, 'FLAG_DOCUMENT_10': vtypes.Boolean, 'FLAG_DOCUMENT_11': vtypes.Boolean, 'FLAG_DOCUMENT_12': vtypes.Boolean, 'FLAG_DOCUMENT_13': vtypes.Boolean, 'FLAG_DOCUMENT_14': vtypes.Boolean, 'FLAG_DOCUMENT_15': vtypes.Boolean, 'FLAG_DOCUMENT_16': vtypes.Boolean, 'FLAG_DOCUMENT_17': vtypes.Boolean, 'FLAG_DOCUMENT_18': vtypes.Boolean, 'FLAG_DOCUMENT_19': vtypes.Boolean, 'FLAG_DOCUMENT_2': vtypes.Boolean, 'FLAG_DOCUMENT_20': vtypes.Boolean, 'FLAG_DOCUMENT_21': vtypes.Boolean, 'FLAG_DOCUMENT_3': vtypes.Boolean, 'FLAG_DOCUMENT_4': vtypes.Boolean, 'FLAG_DOCUMENT_5': vtypes.Boolean, 'FLAG_DOCUMENT_6': vtypes.Boolean, 'FLAG_DOCUMENT_7': vtypes.Boolean, 'FLAG_DOCUMENT_8': vtypes.Boolean, 'FLAG_DOCUMENT_9': vtypes.Boolean, 'FLAG_EMAIL': vtypes.Boolean, 'FLAG_EMP_PHONE': vtypes.Boolean, 'FLAG_MOBIL': vtypes.Boolean, 'FLAG_PHONE': vtypes.Boolean, 'FLAG_WORK_PHONE': vtypes.Boolean, 'LIVE_CITY_NOT_WORK_CITY': vtypes.Boolean, 'LIVE_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REG_CITY_NOT_LIVE_CITY': vtypes.Boolean, 'REG_CITY_NOT_WORK_CITY': vtypes.Boolean, 'REG_REGION_NOT_LIVE_REGION': vtypes.Boolean, 'REG_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REGION_RATING_CLIENT': vtypes.Ordinal, 'REGION_RATING_CLIENT_W_CITY': vtypes.Ordinal, 'HOUR_APPR_PROCESS_START': vtypes.Ordinal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_types = {'NFLAG_LAST_APPL_IN_DAY': vtypes.Boolean, \n",
    "             'NFLAG_INSURED_ON_APPROVAL': vtypes.Boolean}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Create EntitySet from Partition \n",
    "\n",
    "The data has aleady been broken into 50 partitions so we will write a function that takes a single partition and creates the entity set for that data. This can then be passed into a function that calculates the feature matrix from the entityset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path\"\"\"\n",
    "    \n",
    "    app = pd.read_csv('%s/app.csv' % path)\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % path)\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % path)\n",
    "    previous = pd.read_csv('%s/previous.csv' % path)\n",
    "    credit = pd.read_csv('%s/credit.csv' % path)\n",
    "    installments = pd.read_csv('%s/installments.csv' % path)\n",
    "    cash = pd.read_csv('%s/cash.csv' % path)\n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    # Entities with a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR',\n",
    "                                  variable_types = app_types)\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV',\n",
    "                                  variable_types = previous_types)\n",
    "\n",
    "    # Entities that do not have a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                  make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                                  make_index = True, index = 'cash_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                                  make_index = True, index = 'installments_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                                  make_index = True, index = 'credit_index')\n",
    "    \n",
    "    # Relationship between app_train and bureau\n",
    "    r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationship between bureau and bureau balance\n",
    "    r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "    # Relationship between current app and previous apps\n",
    "    r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationships between previous apps and cash, installments, and credit\n",
    "    r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "    r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "    r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "    \n",
    "    # Add in the defined relationships\n",
    "    es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                               r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function to make sure it can make an `EntitySet` from a data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: clients\n",
       "  Entities:\n",
       "    app [Rows: 7125, Columns: 122]\n",
       "    bureau [Rows: 33478, Columns: 17]\n",
       "    previous [Rows: 32795, Columns: 37]\n",
       "    bureau_balance [Rows: 346658, Columns: 4]\n",
       "    cash [Rows: 197046, Columns: 8]\n",
       "    installments [Rows: 264809, Columns: 8]\n",
       "    credit [Rows: 74219, Columns: 23]\n",
       "  Relationships:\n",
       "    bureau.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU\n",
       "    previous.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    cash.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    installments.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> previous.SK_ID_PREV"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es1 = entityset_from_partition('../input/partitions/p1')\n",
    "es1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works! The next step is to create a feature matrix for the partition from the `EntitySet` and the `features`. \n",
    "\n",
    "## Function to Create Featurematrix from EntitySet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es, feature_names):\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature names\"\"\"\n",
    "\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_names, \n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = 100)\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntitySet scattered to workers in 10.319 seconds\n",
      "\n",
      "Elapsed: 00:00 | Remaining: ? | Progress:   0%|          | Calculated: 0/8 chunks\u001b[A\n",
      "Elapsed: 03:53 | Remaining: 27:17 | Progress:  12%|█▎        | Calculated: 1/8 chunks\u001b[A\n",
      "Elapsed: 05:14 | Remaining: 18:46 | Progress:  25%|██▌       | Calculated: 2/8 chunks\u001b[A\n",
      "Elapsed: 05:14 | Remaining: 10:58 | Progress:  38%|███▊      | Calculated: 3/8 chunks\u001b[A\n",
      "Elapsed: 05:16 | Remaining: 06:10 | Progress:  50%|█████     | Calculated: 4/8 chunks\u001b[A\n",
      "Elapsed: 05:16 | Remaining: 02:09 | Progress:  75%|███████▌  | Calculated: 6/8 chunks\u001b[A\n",
      "Elapsed: 05:17 | Remaining: 00:45 | Progress:  88%|████████▊ | Calculated: 7/8 chunks\u001b[A\n",
      "Elapsed: 05:17 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 8/8 chunks\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7125, 1820)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm1 = feature_matrix_from_entityset(es1, featurenames)\n",
    "fm1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the parts needed to create our feature matrixes. The last step is to get Dask to run this in parallel. For the dask implementation, we'll set `n_jobs = 1` and `verbose = 0` because Dask already runs the calculation in parallel and because Dask should have a progress bar that we can view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "We will use the Dask utility `delayed` to parallelize the operation. We iterate through each path in a list of the partitions and tell dask to first create the entity set from the partition, then create the featurematrix from the entityset, and finally, append the feature matrix to a list of feature matrices. The last step is to `concat`enate all of the feature matrices together to get one final matrix that we save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/partitions/p4',\n",
       " '../input/partitions/p3',\n",
       " '../input/partitions/p2',\n",
       " '../input/partitions/p5',\n",
       " '../input/partitions/p19',\n",
       " '../input/partitions/p26',\n",
       " '../input/partitions/p21',\n",
       " '../input/partitions/p28']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['../input/partitions/%s' % file for file in os.listdir('../input/partitions/')]\n",
    "paths[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('concat-4fe40087-a5b1-4847-8f5e-b4026d2b2554')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty list of feature matrices\n",
    "fms = []\n",
    "\n",
    "# Iterate through the paths\n",
    "for path in paths:\n",
    "    if '.DS_Store' in path:\n",
    "        next\n",
    "    else:\n",
    "        # Make the entityset\n",
    "        es = delayed(entityset_from_partition)(path)\n",
    "\n",
    "        # Make the feature matrix\n",
    "        fm = delayed(feature_matrix_from_entityset)(es, feature_names = featurenames)\n",
    "        fms.append(fm)\n",
    "    \n",
    "# Final operation will be to concatenate together all of the feature matrices\n",
    "X = delayed(pd.concat)(fms, axis = 0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[######                                  ] | 15% Completed |  3hr 23min  9.4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/featuretools/computational_backends/utils.py:102: UserWarning: Chunk size is greater than size of feature matrix\n",
      "  warnings.warn(\"Chunk size is greater than size of feature matrix\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 21hr 13min  7.1s\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "# Context for progress and profiling\n",
    "with ProgressBar(), Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "    feature_matrix = X.compute()\n",
    "\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356255, 1820)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature matrix\n",
    "feature_matrix.to_csv('../input/feature_matrix.csv', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations of Run\n",
    "\n",
    "We can use Bokeh and the built in plotting capabilities of Dask to plot the resources used during the computation. This is not necessary for the project but might be interesting for understanding how Dask operates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"8ea090e1-5ad1-40f3-8889-f8f8de9b9e62\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"8ea090e1-5ad1-40f3-8889-f8f8de9b9e62\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"8ea090e1-5ad1-40f3-8889-f8f8de9b9e62\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '8ea090e1-5ad1-40f3-8889-f8f8de9b9e62' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"8ea090e1-5ad1-40f3-8889-f8f8de9b9e62\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"8ea090e1-5ad1-40f3-8889-f8f8de9b9e62\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"8ea090e1-5ad1-40f3-8889-f8f8de9b9e62\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '8ea090e1-5ad1-40f3-8889-f8f8de9b9e62' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"8ea090e1-5ad1-40f3-8889-f8f8de9b9e62\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-4a1ed08d4cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/dask/diagnostics/profile.py\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mprofile_visualize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/dask/diagnostics/profile_visualize.py\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(profilers, file_path, show, save, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \"\"\"\n\u001b[1;32m    170\u001b[0m     \u001b[0mbp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bokeh.plotting'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BOKEH_MISSING_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_notebook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_state'"
     ]
    }
   ],
   "source": [
    "prof.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/prof.txt', 'w') as f:\n",
    "    f.write(str(prof.results))\n",
    "    \n",
    "with open('../input/rprof.txt', 'w') as f:\n",
    "    f.write(str(rprof.results))\n",
    "    \n",
    "with open('../input/cprof.txt', 'w') as f:\n",
    "    f.write(str(cprof.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we used Dask to complete an operation that normally would have taken far more resources than we have available on our personal computer. Although featuretools supports parallel processing, there are still some issues that need to be worked out, and we can take matters into our own hands to get the job done! This will allow anyone with reasonable hardware to take advantage of featuretools! The next notebook is Sampling and Feature Selection where we limit reduce the numbers of features to allow for reasonable modeling times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
