{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: Manual vs Automated Feature Engineering,\n",
    "\n",
    "In this notebook, we will compare the manual, semi-automated, and fully automated (featuretools) feature engineering approaches for the Kaggle Home Credit Default Risk competition. For comparison we will focus on time: how long it took to make the features, and performance: the score in cross validation and when submitted to the Kaggle leaderboard.\n",
    "\n",
    "| Dataset                                       \t| Total Features  before feature selection \t| Total Features  after feature selection \t| Time Spent (conservative estimate)15 minutes7.5 hours (0.067 features,per minute)10 hours (1.96 featuresper minute)2 hours (23.63 featuresper minute) \t| CV ROC AUC default model \t| CV ROC AUC  Optimized Model \t| Public Leaderboard ROC AUC optimized model \t|\n",
    "|-----------------------------------------------\t|------------------------------------------\t|-----------------------------------------\t|-------------------------------------------------------------------------------------------------------------------------------------------------------\t|--------------------------\t|-----------------------------\t|--------------------------------------------\t|\n",
    "| Baseline  (Main table after one-hot encoding) \t| 241                                      \t| 203                                     \t| 15 minutes                                                                                                                                            \t| 0.754497 (0.00600176)    \t| 0.759222  (0.00522168)      \t| 0.745                                      \t|\n",
    "| Manual Feature Engineering                    \t| 271 (30 from  manual engineering)        \t| 231 (26 from  manual engineering)       \t| 16 hours                                                                                                                                              \t| 0.772477  (0.00440231)   \t| 0.78036  (0.00449839)       \t| 0.786                                      \t|\n",
    "| Manual + Semi-Automated                       \t| 1444 (1173 from  semi-auto methods)      \t| 880 (657 from  semi-auto methods)       \t| 20 hours                                                                                                                                              \t| 0.779672 (0.00566087)    \t| 0.788476 (0.00435772)       \t| 0.791                                      \t|\n",
    "| Fully Automated                               \t| 2091 (1850 from  featuretools)           \t| 1042 (859 from featuretools)            \t| 1 hour                                                                                                                                                \t| 0.774754 (0.00151319)    \t| 0.782407  (0.00120016)      \t| 0.787                                      \t|\n",
    "  \n",
    "## Explanation of Result Categories\n",
    "   \n",
    "* __Dataset__: refers to the method used to construct the set of features. The baseline set is the main dataframe (`app`) after one-hot encoding categorical variables\n",
    "* __Total Features before feature selection__: the total number of predictor variables after implementing the method. Numbers in parenthesis indicate the features built by the method alone since each method built on the previous\n",
    "* __Total Features after feature selection__: same as the previous column except the metric after feature selection\n",
    "* __Time Spent__: Total time spent creating the set of features. This is a __conservative__ estimate as it does not include the hundreds of hours spent by other data scientists working on the problem or the hours I personally spent reading about the problem. This refers only to the time I spent actively coding the method.\n",
    "* __CV ROC AUC default model__. The 5-fold cross validation ROC AUC using the default hyperparameter values of the Gradient Boosting Machine (GBM) implemented with the LightGBM library. The number of estimators was found using 100 rounds of early stopping with 5-fold cv. (Number in parenthesis is the standard deviation across five folds).\n",
    "* __CV ROC AUC optimized model__. The 5-fold cross validation ROC AUC using the best hyperparameters from random search for 100 iterations on the respective sample of data. (Number in parenthesis is the standard deviation)\n",
    "* __Public Leaderboard ROC AUC__. The ROC AUC score of dataset from the GBM model when submitted to the public leaderboard on Kaggle. The GBM model used the optimal hyperparameters and early stopping for the number of estimators. Predictions were made on the testing data and then uploaded to Kaggle where the Public Leaderboard is calculated using 10% of the total testing observations. The final leaderboard will be made known at the end of the competition. \n",
    " \n",
    "# Methodology\n",
    "    \n",
    "To assess the features, we want to perform several operations:,\n",
    "    \n",
    "1. Cross Validation (5 folds) ROC AUC with default GBM model in LightGBM library\n",
    "2. Cross Validation (5 folds) ROC AUC with best hyperparameters from 100 iterations of random search on data sample\n",
    "3. Public leaderboard ROC AUC from submitting predictions on testing data to Kaggle\n",
    "4. Correlations with the label (`TARGET`)\n",
    "5. Feature importances in the trained model\n",
    "    \n",
    "## Random Search\n",
    "   \n",
    "The \"optimal\" hyperparameters of the GBM for each dataset were found by applying 100 iterations of random search to a sample of 10% of each set of training data. Performance was measured by the 5-fold cross validation ROC AUC using early stopping to determine the number of estimators to train. The gradient boosting machine was implemented in LightGBM. In addition to testing with the optimal hyperparameter values, we will assess the cross validation using the default hyperparameters to determine the relative effects of hyperparameter tuning versus feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap \n",
    "\n",
    "To apply the same operations to the three datasets, we create a function that calculate the 5 metrics above. This function will take in the feature matrix and the hyperparameter tuning results and return the five metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Evaluating dictionary\n",
    "import ast\n",
    "\n",
    "# Utilities developed in previous notebooks\n",
    "from utils import format_data, plot_feature_importances, evaluate\n",
    "\n",
    "RSEED = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Number of Features\n",
    "\n",
    "The first order of business is to compare the number of features created by each method that remain after feature selection. We can do this by loading in the first row of the data and comparing the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = pd.read_csv('../input/application_train.csv'), pd.read_csv('../input/application_test.csv')\n",
    "test['TARGET'] = np.nan\n",
    "train, test = pd.get_dummies(train).align(pd.get_dummies(test), axis = 1, join = 'inner')\n",
    "\n",
    "original_features = list(train.columns)\n",
    "manual_features = [x for x in pd.read_csv('../input/features_manual_selected.csv', nrows = 1).columns if x not in original_features]\n",
    "auto_features = [x for x in pd.read_csv('../input/feature_matrix_select.csv', nrows = 1).columns if x not in original_features and x not in manual_features]\n",
    "semi_features = [x for x in pd.read_csv('../input/features_semi_selected.csv', nrows = 1).columns if x not in original_features and x not in manual_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were originally 241 features.\n",
      "26 Manual Features remained after feature selection.\n",
      "859 Automated Features remained after feature selection.\n",
      "657 Semi-Automated Features remained after feature selection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There were originally {} features.\".format(len(original_features) - 2))\n",
    "print(\"{} Manual Features remained after feature selection.\".format(len(manual_features)))\n",
    "print(\"{} Automated Features remained after feature selection.\".format(len(auto_features)))\n",
    "print(\"{} Semi-Automated Features remained after feature selection.\".format(len(semi_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  203\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input data must be 2 dimensional and non empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6f7a1c2cc1f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhyp_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../results/rs_features_default_sample_finished.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ft-manual-comparison/kaggle_home_credit/notebooks/utils.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(fm, hyp_results)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# Make predictions on testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# Make submission dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \"\"\"\n\u001b[1;32m    741\u001b[0m         result = super(LGBMClassifier, self).predict(X, raw_score, num_iteration,\n\u001b[0;32m--> 742\u001b[0;31m                                                      pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m                              % (self._n_features, n_features))\n\u001b[1;32m    527\u001b[0m         return self.booster_.predict(X, raw_score=raw_score, num_iteration=num_iteration,\n\u001b[0;32m--> 528\u001b[0;31m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, pred_parameter, **kwargs)\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_iteration\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m             \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_has_header\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_reshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_leaf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot use Dataset instance for prediction, please use raw data instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_data_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mpredict_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC_API_PREDICT_NORMAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input data must be 2 dimensional and non empty.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'auto'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input data must be 2 dimensional and non empty."
     ]
    }
   ],
   "source": [
    "fm = pd.read_csv('../input/features_default_selected.csv')\n",
    "hyp_results = pd.read_csv('../results/rs_features_default_sample_finished.csv', index_col=0)\n",
    "\n",
    "results, feature_importances, submission = evaluate(fm, hyp_results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../results/default_results.csv', index = False)\n",
    "submission.to_csv('../submissions/default_submission.csv', index = False)\n",
    "\n",
    "norm_feature_importances = plot_feature_importances(feature_importances)\n",
    "norm_feature_importances.to_csv('../results/default_fi.csv', index = False)\n",
    "norm_feature_importances.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model scores __0.745__ when submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('../input/features_manual_selected.csv')\n",
    "hyp_results = pd.read_csv('../results/rs_features_manual_sample_finished.csv', index_col=0)\n",
    "\n",
    "results, feature_importances, submission = evaluate(fm, hyp_results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../results/manual_results.csv', index = False)\n",
    "submission.to_csv('../submissions/manual_submission.csv', index = False)\n",
    "\n",
    "norm_feature_importances = plot_feature_importances(feature_importances)\n",
    "norm_feature_importances.to_csv('../results/manual_fi.csv', index = False)\n",
    "norm_feature_importances.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These score __0.786__ when submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Features Using Featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('../input/feature_matrix_select.csv')\n",
    "hyp_results = pd.read_csv('../results/rs_feature_matrix_sample_finished.csv', index_col=0)\n",
    "\n",
    "results, feature_importances, submission = evaluate(fm, hyp_results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../results/auto_results.csv', index = False)\n",
    "submission.to_csv('../submissions/auto_submission.csv', index = False)\n",
    "\n",
    "norm_feature_importances = plot_feature_importances(feature_importances)\n",
    "norm_feature_importances.to_csv('../results/auto_fi.csv', index = False)\n",
    "norm_feature_importances.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features score __0.787__ when submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Automated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('../input/features_semi_selected.csv')\n",
    "hyp_results = pd.read_csv('../results/rs_features_semi_sample_finished.csv', index_col=0)\n",
    "\n",
    "results, feature_importances, submission = evaluate(fm, hyp_results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Automated Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../results/semi_results.csv', index = False)\n",
    "submission.to_csv('../submissions/semi_submission.csv', index = False)\n",
    "\n",
    "norm_feature_importances = plot_feature_importances(feature_importances)\n",
    "norm_feature_importances.to_csv('../results/semi_fi.csv', index = False)\n",
    "norm_feature_importances.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset scores __0.791__ when submitted to the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "With the results presented below, this project is now complete. What we have learned is that with only a few minutes of programming, Featuretools is able to get us to a level comparable with dozens of hours of manual feature engineering on a real-world data science problem.\n",
    "\n",
    "| Dataset                                       \t| Total Features  before feature selection \t| Total Features  after feature selection \t| Time Spent (conservative estimate)15 minutes7.5 hours (0.067 features,per minute)10 hours (1.96 featuresper minute)2 hours (23.63 featuresper minute) \t| CV ROC AUC default model \t| CV ROC AUC  Optimized Model \t| Public Leaderboard ROC AUC optimized model \t|\n",
    "|-----------------------------------------------\t|------------------------------------------\t|-----------------------------------------\t|-------------------------------------------------------------------------------------------------------------------------------------------------------\t|--------------------------\t|-----------------------------\t|--------------------------------------------\t|\n",
    "| Baseline  (Main table after one-hot encoding) \t| 241                                      \t| 203                                     \t| 15 minutes                                                                                                                                            \t| 0.754497 (0.00600176)    \t| 0.759222  (0.00522168)      \t| 0.745                                      \t|\n",
    "| Manual Feature Engineering                    \t| 271 (30 from  manual engineering)        \t| 231 (26 from  manual engineering)       \t| 16 hours                                                                                                                                              \t| 0.772477  (0.00440231)   \t| 0.78036  (0.00449839)       \t| 0.786                                      \t|\n",
    "| Manual + Semi-Automated                       \t| 1444 (1173 from  semi-auto methods)      \t| 880 (657 from  semi-auto methods)       \t| 20 hours                                                                                                                                              \t| 0.779672 (0.00566087)    \t| 0.788476 (0.00435772)       \t| 0.791                                      \t|\n",
    "| Fully Automated                               \t| 2091 (1850 from  featuretools)           \t| 1042 (859 from featuretools)            \t| 1 hour                                                                                                                                                \t| 0.774754 (0.00151319)    \t| 0.782407  (0.00120016)      \t| 0.787                                      \t|\n",
    "\n",
    "The takeaways from this project are:\n",
    "\n",
    "* Featuretools is able to achieve results comparable or better than manual feature engineering with a 10x gain in speed\n",
    "* Feature engineering undoubtedly adds value to the problem and is a necessity in a data science pipeline\n",
    "* Model hyperparameter tuning also has an affect on model performance\n",
    "* Featuretools is more broadly applicable to data science problems and allows for more efficient workflows \n",
    "\n",
    "Feature engineering is a must, and a dream of machine learning practicioners is a method for automated feature engineering that would be able to extract useful features from a set of related tables. Featuretools is not perfect, but it represents a step in the right direction and can serve as a valuable aid to the data scientist in the crucial task of feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
