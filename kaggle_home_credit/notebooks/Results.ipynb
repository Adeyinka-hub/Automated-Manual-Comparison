{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: Manual vs Automated Feature Engineering,\n",
    "\n",
    "In this notebook, we will compare the manual, semi-automated, and fully automated (featuretools) feature engineering approaches for the Kaggle Home Credit Default Risk competition. For comparison we will focus on time: how long it took to make the features, and performance: the score in cross validation and when submitted to the Kaggle leaderboard.\n",
    "  \n",
    "## Explanation of Result Categories\n",
    "   \n",
    "* __Method__: refers to the method used to construct the set of features. The baseline set is the main dataframe (`app`) after one-hot encoding categorical variables,\n",
    "* __Total Features__: the total number of predictor variables after implementing the method. Numbers in parenthesis indicate the features built by the method alone since each method built on the previous,\n",
    "* __Time Spent__: Total time spent creating the set of features. This is a __conservative__ estimate as it does not include the hundreds of hours spent by other data scientists working on the problem or the hours I personally spent reading about the problem. This refers only to the time I spent actively coding the technique.,\n",
    "* __CV ROC AUC default model__. The 5-fold cross validation ROC AUC using the default hyperparameter values of the Gradient Boosting Machine (GBM) implemented with the LightGBM library. The number of estimators was found using 100 rounds of early stopping with the 5-fold cv.,\n",
    "* __Public Leaderboard ROC AUC default model__. The ROC AUC score of dataset from the GBM model when submitted to the public leaderboard on Kaggle. The GBM model used the same default hyperparameters and the cv early stopping results for the number of estimators. Predictions were made on the testing data and then uploaded to Kaggle where the Public Leaderboard is calculated using 10% of the total testing observations. The final leaderboard will be made known at the end of the competition. ,\n",
    "* __CV ROC AUC optimized model__. The 5-fold cross validation ROC AUC using the best hyperparameters from random search for 150 iterations. ,\n",
    "* __Public Leaderboard ROC AUC optimized model__. The ROC AUC score when submitted to the Kaggle competition using the hyperparameters from random search,\n",
    " \n",
    "# Methodology\n",
    "    \n",
    "To assess the features, we want to perform several operations:,\n",
    "    \n",
    "1. Cross Validation (5 folds) ROC AUC with default GBM model in LightGBM library\n",
    "2. Cross Validation (5 folds) ROC AUC with best hyperparameters from 100 iterations of random search on data sample\n",
    "3. Public leaderboard ROC AUC from submitting predictions on testing data to Kaggle\n",
    "4. Correlations with the label (`TARGET`)\n",
    "5. Feature importances in the trained model\n",
    "    \n",
    "## Random Search\n",
    "   \n",
    "The \"optimal\" hyperparameters of the GBM for each dataset were found by applying 100 iterations of random search to a sample of 10% of each set of training data. Performance was measured by the 5-fold cross validation ROC AUC using early stopping to determine the number of estimators to train. The gradient boosting machine was implemented in LightGBM. In addition to testing with the optimal hyperparameter values, we will assess the cross validation using the default hyperparameters to determine the relative effects of hyperparameter tuning versus feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap \n",
    "\n",
    "To apply the same operations to the three datasets, we create a function that calculate the 5 metrics above. This function will take in the feature matrix and the hyperparameter tuning results and return the five metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import ast\n",
    "\n",
    "from utils import format_data, plot_feature_importances\n",
    "\n",
    "RSEED = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_results = pd.read_csv('../results/rs_feature_matrix_sample.csv_finished.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_results = hyp_results.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "\n",
    "best_hyp = ast.literal_eval(hyp_results.loc[0, 'params'])\n",
    "best_random_score = hyp_results.loc[0, 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(features):\n",
    "    \"\"\"Format a set of training and testing features joined together\n",
    "       into separate sets for machine learning\"\"\"\n",
    "    \n",
    "    train = features[features['TARGET'].notnull()].copy()\n",
    "    test = features[features['TARGET'].isnull()].copy()\n",
    "    \n",
    "    train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n",
    "    test_ids = list(test['SK_ID_CURR'])\n",
    "    \n",
    "    train = train.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
    "    test = test.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
    "    \n",
    "    feature_names = list(train.columns)\n",
    "    \n",
    "    return train, train_labels, test, test_ids, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('../input/features_manual_selected.csv')\n",
    "# train, train_labels, test, test_ids, feature_names = format_data(fm)\n",
    "# train_set = lgb.Dataset(train, label = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['default_auc', 'default_auc_std', \n",
    "                                  'opt_auc', 'opt_auc_std', \n",
    "                                  'random_search_auc'], index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "default_hyp = model.get_params()\n",
    "del default_hyp['n_estimators'], default_hyp['silent']\n",
    "\n",
    "cv_results = lgb.cv(default_hyp, train_set, nfold = 5, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', seed = RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_auc = cv_results['auc-mean'][-1]\n",
    "default_auc_std = cv_results['auc-stdv'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del best_hyp['n_estimators']\n",
    "\n",
    "cv_results = lgb.cv(best_hyp, train_set, nfold = 5, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', seed = RSEED)\n",
    "\n",
    "opt_auc = cv_results['auc-mean'][-1]\n",
    "opt_auc_std = cv_results['auc-stdv'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_n_estimators = len(cv_results['auc-mean'])\n",
    "model = lgb.LGBMClassifier(n_estimators = opt_n_estimators, **best_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[0, 'default_auc'] = default_auc\n",
    "results.loc[0, 'default_auc_std'] = default_auc_std\n",
    "results.loc[0, 'random_search_auc'] = best_random_score\n",
    "results.loc[0, 'opt_auc'] = opt_auc\n",
    "results.loc[0, 'opt_auc_std'] = opt_auc_std\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on whole training set\n",
    "model.fit(train, train_labels)\n",
    "\n",
    "# Make predictions on testing data\n",
    "preds = model.predict_proba(test)[:, 1]\n",
    "\n",
    "# Make submission dataframe\n",
    "submission = pd.DataFrame({'SK_ID_CURR': test_ids, \n",
    "                           'TARGET': preds})\n",
    "\n",
    "feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                    'importance': model.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(fm, hyp_results):\n",
    "    \"\"\"Evaluate a feature matrix using the hyperparameter tuning results.\n",
    "    \n",
    "    Parameters:\n",
    "        fm (dataframe): feature matrix with observations in the rows and features in the columns. This will\n",
    "                        be passed to `format_data` and hence must have a train set where the `TARGET` values are \n",
    "                        not null and a test set where `TARGET` is null. Must also have the `SK_ID_CURR` column.\n",
    "        \n",
    "        hyp_results (dataframe): results from hyperparameter tuning. Must have column `score` (where higher is better)\n",
    "                                 and `params` holding the model hyperparameters\n",
    "                                 \n",
    "    Returns:\n",
    "        results (dataframe): the cross validation roc auc from the default hyperparameters and the \n",
    "                             optimal hyperparameters\n",
    "        \n",
    "        feature_importances (dataframe): feature importances from the gradient boosting machine. Columns are \n",
    "                                          `feature` and `importance`. This can be used in `plot_feature_importances`.\n",
    "                                          \n",
    "        submission (dataframe): Predictions which can be submitted to the Kaggle Home Credit competition. Save\n",
    "                                these as `submission.to_csv(\"filename.csv\", index = False)` and upload\n",
    "       \"\"\"\n",
    "    \n",
    "    # Format the feature matrix \n",
    "    train, train_labels, test, test_ids, feature_names = format_data(fm)\n",
    "    \n",
    "    # Training set \n",
    "    train_set = lgb.Dataset(train, label = train_labels)\n",
    "\n",
    "    # Dataframe to hold results\n",
    "    results = pd.DataFrame(columns = ['default_auc', 'default_auc_std', \n",
    "                                      'opt_auc', 'opt_auc_std', \n",
    "                                      'random_search_auc'], index = [0])\n",
    "\n",
    "    # Create a default model and find the hyperparameters\n",
    "    model = lgb.LGBMClassifier()\n",
    "    default_hyp = model.get_params()\n",
    "    \n",
    "    # Remove n_estimators because this is found through early stopping\n",
    "    del default_hyp['n_estimators'], default_hyp['silent']\n",
    "\n",
    "    # Cross validation with default hyperparameters\n",
    "    default_cv_results = lgb.cv(default_hyp, train_set, nfold = 5, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                                metrics = 'auc', seed = RSEED)\n",
    "    \n",
    "    default_auc = default_cv_results['auc-mean'][-1]\n",
    "    default_auc_std = default_cv_results['auc-stdv'][-1]\n",
    "    \n",
    "    # Locate the optimal hyperparameters\n",
    "    hyp_results = hyp_results.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "    best_hyp = ast.literal_eval(hyp_results.loc[0, 'params'])\n",
    "    best_random_score = hyp_results.loc[0, 'score']\n",
    "\n",
    "    del best_hyp['n_estimators']\n",
    "\n",
    "    # Cross validation with best hyperparameter values\n",
    "    opt_cv_results = lgb.cv(best_hyp, train_set, nfold = 5, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                            metrics = 'auc', seed = RSEED)\n",
    "\n",
    "    opt_auc = opt_cv_results['auc-mean'][-1]\n",
    "    opt_auc_std = opt_cv_results['auc-stdv'][-1]\n",
    "    \n",
    "    # Insert results into dataframe\n",
    "    results.loc[0, 'default_auc'] = default_auc\n",
    "    results.loc[0, 'default_auc_std'] = default_auc_std\n",
    "    results.loc[0, 'random_search_auc'] = best_random_score\n",
    "    results.loc[0, 'opt_auc'] = opt_auc\n",
    "    results.loc[0, 'opt_auc_std'] = opt_auc_std\n",
    "    \n",
    "    # Extract the optimum number of estimators\n",
    "    opt_n_estimators = len(opt_cv_results['auc-mean'])\n",
    "    model = lgb.LGBMClassifier(n_estimators = opt_n_estimators, **best_hyp)\n",
    "    \n",
    "    # Fit on whole training set\n",
    "    model.fit(train, train_labels)\n",
    "\n",
    "    # Make predictions on testing data\n",
    "    preds = model.predict_proba(test)[:, 1]\n",
    "\n",
    "    # Make submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, \n",
    "                               'TARGET': preds})\n",
    "\n",
    "    # Make feature importances dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                        'importance': model.feature_importances_})\n",
    "\n",
    "    return results, feature_importances, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, feature_importances, submission = evaluate(fm, hyp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
