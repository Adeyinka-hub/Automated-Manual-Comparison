{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: Manual vs Automated Feature Engineering\n",
    "\n",
    "In this notebook, we will compare the manual, semi-automated, and fully automated (featuretools) feature engineering approaches for the Kaggle Home Credit Default Risk competition. For comparison we will focus on time: how long it took to make the features, and performance: the score in cross validation and when submitted to the Kaggle leaderboard.\n",
    "\n",
    "The summary of results are as follows:\n",
    "\n",
    "| Dataset                     \t| Total Features                      \t| Time Spent (conservative estimate)  \t| CV ROC AUC default model \t| Public Leaderboard  ROC AUC default model \t|\n",
    "|-----------------------------\t|-------------------------------------\t|-------------------------------------\t|--------------------------\t|-------------------------------------------\t|\n",
    "| Main after one-hot encoding \t| 241                                 \t| 15 minutes                          \t| 0.75565                  \t| 0.741                                     \t|\n",
    "| Manual Feature Engineering  \t| 271 (30 from  manual engineering)   \t| 7.5 hours (15 minutes per feature)  \t| 0.77227                  \t| 0.776                                     \t|\n",
    "| Manual + Semi-Automated     \t| 1444 (1173 from  semi-auto methods) \t| 10 hours (0.5 minutes per feature)  \t| 0.77987                  \t| 0.782                                     \t|\n",
    "| Fully Automated             \t| 2800 (2574 from  featuretools)      \t| 2 hours (0.05 minutes per  feature) \t|                          \t|                                           \t|\n",
    "\n",
    "## Explanation of Result Categories\n",
    "\n",
    "* __Dataset__: refers to the method used to construct the set of features. The baseline set is the main dataframe (`app`) after one-hot encoding categorical variables\n",
    "* __Total Features__: the total number of predictor variables after implementing the method. Numbers in parenthesis indicate the features built by the method alone since each method built on the previous\n",
    "* __Time Spent__: Total time spent creating the set of features. This is a __conservative__ estimate as it does not include the hundreds of hours spent by other data scientists working on the problem or the hours I personally spent reading about the problem. This refers only to the time I spent actively coding the technique.\n",
    "* __CV ROC AUC default model__. The 5-fold cross validation ROC AUC using the default hyperparameter values of the Gradient Boosting Machine (GBM) implemented with the LightGBM library. The number of estimators was found using 100 rounds of early stopping with the 5-fold cv.\n",
    "* __Public Leaderboard ROC AUC default model__. The ROC AUC score of dataset from the GBM model when submitted to the public leaderboard on Kaggle. The GBM model used the same default hyperparameters and the cv early stopping results for the number of estimators. Predictions were made on the testing data and then uploaded to Kaggle where the Public Leaderboard is calculated using 10% of the total testing observations. The final leaderboard will be made known at the end of the competition. \n",
    "\n",
    "## Random Search\n",
    "\n",
    "After using the default hyperparameters of the GBM in LightGBM as a first approximation of the dataset performance, random search was run for 150 iterations on the manual feature engineering dataset (saved as `features_manual_domain.csv`). These results are available in `random_search_domain.csv`. Below, we go through the results of random search and then use the best hyperparameter values to assess cross validation ROC AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RSEED = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>iteration</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.780678</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': True, 'boosting...</td>\n",
       "      <td>125</td>\n",
       "      <td>384.011306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780417</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': True, 'boosting...</td>\n",
       "      <td>109</td>\n",
       "      <td>370.445021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.780216</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': True, 'boosting...</td>\n",
       "      <td>9</td>\n",
       "      <td>137.456086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.780154</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': True, 'boosting...</td>\n",
       "      <td>41</td>\n",
       "      <td>134.778583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.780134</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': True, 'boosting...</td>\n",
       "      <td>97</td>\n",
       "      <td>169.808482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.780001</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': True, 'boosting...</td>\n",
       "      <td>7</td>\n",
       "      <td>365.550064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.779729</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': True, 'boosting...</td>\n",
       "      <td>62</td>\n",
       "      <td>122.443247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.779705</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': False, 'boostin...</td>\n",
       "      <td>135</td>\n",
       "      <td>143.399460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.779693</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': False, 'boostin...</td>\n",
       "      <td>126</td>\n",
       "      <td>136.182787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.779674</td>\n",
       "      <td>{'verbose': 1, 'is_unbalance': True, 'boosting...</td>\n",
       "      <td>75</td>\n",
       "      <td>244.119122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                                    hyperparameters  iteration  \\\n",
       "0  0.780678  {'verbose': 1, 'is_unbalance': True, 'boosting...        125   \n",
       "1  0.780417  {'verbose': 1, 'is_unbalance': True, 'boosting...        109   \n",
       "2  0.780216  {'verbose': 1, 'is_unbalance': True, 'boosting...          9   \n",
       "3  0.780154  {'verbose': 1, 'is_unbalance': True, 'boosting...         41   \n",
       "4  0.780134  {'verbose': 1, 'is_unbalance': True, 'boosting...         97   \n",
       "5  0.780001  {'verbose': 1, 'is_unbalance': True, 'boosting...          7   \n",
       "6  0.779729  {'verbose': 1, 'is_unbalance': True, 'boosting...         62   \n",
       "7  0.779705  {'verbose': 1, 'is_unbalance': False, 'boostin...        135   \n",
       "8  0.779693  {'verbose': 1, 'is_unbalance': False, 'boostin...        126   \n",
       "9  0.779674  {'verbose': 1, 'is_unbalance': True, 'boosting...         75   \n",
       "\n",
       "         time  \n",
       "0  384.011306  \n",
       "1  370.445021  \n",
       "2  137.456086  \n",
       "3  134.778583  \n",
       "4  169.808482  \n",
       "5  365.550064  \n",
       "6  122.443247  \n",
       "7  143.399460  \n",
       "8  136.182787  \n",
       "9  244.119122  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random = pd.read_csv('../input/results/random_search_domain.csv', index_col=0)\n",
    "\n",
    "# Sort with best values on top\n",
    "random = random.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "random.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the best cross validation ROC AUC before random search was 0.77227. The random search significantly improved the cross validation score. \n",
    "\n",
    "We can look at the best hyperparameter values and then use these to train a model for any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'gbdt',\n",
      " 'colsample_bytree': 0.6,\n",
      " 'is_unbalance': True,\n",
      " 'learning_rate': 0.005284379855924019,\n",
      " 'min_child_samples': 470,\n",
      " 'num_leaves': 100,\n",
      " 'reg_alpha': 0.836734693877551,\n",
      " 'reg_lambda': 0.7755102040816326,\n",
      " 'subsample': 0.8888888888888888,\n",
      " 'subsample_for_bin': 260000}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pprint\n",
    "\n",
    "best_hyp = ast.literal_eval(random.loc[0, 'hyperparameters'])\n",
    "del best_hyp['n_estimators'], best_hyp['verbose'], best_hyp['metric']\n",
    "\n",
    "pprint.pprint(best_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a model from these hyperparameters and assess the  5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(features):\n",
    "    \"\"\"Format a set of training and testing features joined together\n",
    "       into separate sets for machine learning\"\"\"\n",
    "    \n",
    "    train = features[features['TARGET'].notnull()].copy()\n",
    "    test = features[features['TARGET'].isnull()].copy()\n",
    "    \n",
    "    train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n",
    "    test_ids = list(test['SK_ID_CURR'])\n",
    "    \n",
    "    train = train.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
    "    test = test.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
    "    \n",
    "    feature_names = list(train.columns)\n",
    "    \n",
    "    return train, train_labels, test, test_ids, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features = pd.read_csv('../input/features_manual_domain.csv')\n",
    "\n",
    "manual_train, train_labels, manual_test, test_ids, manual_feature_names = format_data(manual_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Number of estimators found through early stopping\n",
    "manual_train_set = lgb.Dataset(manual_train, label = train_labels)\n",
    "\n",
    "# 5 Fold Cross Validation\n",
    "manual_cv = lgb.cv(best_hyp, train_set = manual_train_set,  seed = RSEED, nfold = 5, metrics = 'auc', \n",
    "                   early_stopping_rounds = 100, num_boost_round = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_estimators = len(manual_cv['auc-mean'])\n",
    "print('Manual 5-Fold CV ROC AUC: {:.5f} with std: {:.5f}.'.format(\n",
    "                                                        manual_cv['auc-mean'][-1], manual_cv['auc-stdv'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, n = 15, threshold = None):\n",
    "    \"\"\"\n",
    "    Plots n most important features. Also plots the cumulative importance if\n",
    "    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n",
    "    Intended for use with any tree-based feature importances. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe of feature importances. Columns must be \"feature\" and \"importance\"\n",
    "    \n",
    "    n : int, default = 15\n",
    "        Number of most important features to plot\n",
    "    \n",
    "    threshold : float, default = None\n",
    "        Threshold for cumulative importance plot. If not provided, no plot is made\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe ordered by feature importances with a normalized column (sums to 1)\n",
    "        and a cumulative importance column\n",
    "    \n",
    "    Note\n",
    "    --------\n",
    "        * Normalization in this case means sums to 1. \n",
    "        * Cumulative importance is calculated by summing features from most to least important\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    # Bar plot of n most important features\n",
    "    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n",
    "                            x = 'feature', color = 'blue', edgecolor = 'k', figsize = (12, 8),\n",
    "                            legend = False)\n",
    "\n",
    "    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n",
    "    plt.title(f'Top {n} Most Important Features', size = 18)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    if threshold:\n",
    "        # Cumulative importance plot\n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n",
    "        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n",
    "        plt.title('Cumulative Feature Importance', size = 18);\n",
    "        \n",
    "        # Number of features needed for threshold cumulative importance\n",
    "        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "        \n",
    "        # Add vertical line to plot\n",
    "        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.2, linestyles = '--', colors = 'red')\n",
    "        plt.show();\n",
    "        \n",
    "        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, 100 * threshold))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_model = lgb.LGBMClassifier(**best_hyp, n_estimators = manual_estimators)\n",
    "manual_model.fit(manual_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_fi = pd.DataFrame({'feature': manual_feature_names, 'importance': manual_model.feature_importances_})\n",
    "norm_manual_fi = plot_feature_importances(manual_fi, 20)\n",
    "norm_manual_fi.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Automated Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Automated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_features = pd.read_csv('../input/features_semi.csv')\n",
    "\n",
    "semi_train, train_labels, semi_test, test_ids, semi_feature_names = format_data(semi_features)\n",
    "\n",
    "# Number of estimators found through early stopping\n",
    "semi_train_set = lgb.Dataset(semi_train, label = train_labels)\n",
    "\n",
    "# 5 Fold Cross Validation\n",
    "semi_cv = lgb.cv(best_hyp, train_set = semi_train_set,  seed = RSEED, nfold = 5, metrics = 'auc', \n",
    "                   early_stopping_rounds = 100, num_boost_round = 10000)\n",
    "\n",
    "semi_estimators = len(semi_cv['auc-mean'])\n",
    "print('semi 5-Fold CV ROC AUC: {:.5f} with std: {:.5f}.'.format(\n",
    "                                                        semi_cv['auc-mean'][-1], semi_cv['auc-stdv'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_model = lgb.LGBMClassifier(**best_hyp, n_estimators = semi_estimators)\n",
    "semi_model.fit(semi_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_fi = pd.DataFrame({'feature': semi_feature_names, 'importance': semi_model.feature_importances_})\n",
    "norm_semi_fi = plot_feature_importances(semi_fi, 20)\n",
    "norm_semi_fi.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
