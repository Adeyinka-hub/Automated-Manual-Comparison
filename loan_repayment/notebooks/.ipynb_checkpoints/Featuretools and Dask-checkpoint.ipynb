{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools Implementation with Dask\n",
    "\n",
    "A simple run of Deep Feature Synthesis from the Automated Loan Repayment notebook takes about 25 hours on an AWS machine with 64 GB of RAM! Featuretools does have support for parallel processing if you have multiple cores (which nearly every single laptop now does), but it currently sends the entire EntitySet to each process which means you might exhaust the memory on any one core. For example, that AWS machine has 8 GB per core, which might seem like a lot until you realize the EntitySet takes up about 11 GB and setting `n_jobs=-1` will cause an out of memory error. Therefore, we cannot use the parallel processing in Featuretools and instead have to build our own implementation with Dask. Fortunately, options such as Dask make it easy to take advantage of multiple cores on our own machine. In this notebook, we'll see how to run Deep Feature Synthesis in about 3 hours on a personal laptop with 16 GB of RAM. \n",
    "\n",
    "## Roadmap\n",
    "\n",
    "Following is our plan of action for implementing Dask\n",
    "\n",
    "1. Convert `object` data types to `category`\n",
    "    * This reduces memory consumption significantly\n",
    "2. Create 100 partitions of data and save to disk\n",
    "    * Each partition will contain data from all 7 seven tables for 1/100 of the client ids, `SK_ID_CURR`\n",
    "    * Each partition can be used to make an EntitySet and hence a feature matrix\n",
    "3. Write a function to take a partition and create an `EntitySet`\n",
    "4. Write a function to take an `EntitySet` and calculate a `feature_matrix`\n",
    "    * Since we already have the feature names, we can use `ft.calculate_feature_matrix`\n",
    "5. Use Dask with system processes to generate feature matrices for 8 partitions at a time\n",
    "    * Save these subset feature matrices to disk\n",
    "    * Using proceses will start 8 workers, one for each core, with 2 GB of memory each\n",
    "    * We can't generate the entire feature matrix at once using processes because the final feature matrix is too large to fit on a single core\n",
    "6. Use Dask with threads to read in subset feature matrices and create final feature matrix\n",
    "    * Using threads will start 1 worker with 16 GB of memory, enough to hold the entire feature matrix\n",
    "    * Can save this feature matrix to disk for later use in a machine learning pipeline\n",
    "    \n",
    "This might seem like a lot of tasks, but each one is a relatively simple step. At the end, we'll have a working implementation of Dask that lets us take full advantage of our computing resources. While we could solve this whole problem by just renting a larger machine, this approach will give us a chance to learn about how to work with constraints and engineer a solution. Sometimes having too many resources can limit your creativity, and working with constraints forces us to be innovative! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "# Utilities\n",
    "import sys\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df):\n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory before converting types: 4.38 gb.\n",
      "Total memory after converting types: 2.06 gb.\n"
     ]
    }
   ],
   "source": [
    "# Read in the datasets and replace the anomalous values\n",
    "app_train = pd.read_csv('../input/application_train.csv').replace({365243: np.nan})\n",
    "app_test = pd.read_csv('../input/application_test.csv').replace({365243: np.nan})\n",
    "bureau = pd.read_csv('../input/bureau.csv').replace({365243: np.nan})\n",
    "bureau_balance = pd.read_csv('../input/bureau_balance.csv').replace({365243: np.nan})\n",
    "cash = pd.read_csv('../input/POS_CASH_balance.csv').replace({365243: np.nan})\n",
    "credit = pd.read_csv('../input/credit_card_balance.csv').replace({365243: np.nan})\n",
    "previous = pd.read_csv('../input/previous_application.csv').replace({365243: np.nan})\n",
    "installments = pd.read_csv('../input/installments_payments.csv').replace({365243: np.nan})\n",
    "\n",
    "app_test['TARGET'] = np.nan\n",
    "\n",
    "# Join together training and testing\n",
    "app = app_train.append(app_test, ignore_index = True, sort = True)\n",
    "\n",
    "# Need `SK_ID_CURR` in every dataset\n",
    "bureau_balance = bureau_balance.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], \n",
    "                                      on = 'SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "print(f\"\"\"Total memory before converting types: \\\n",
    "{round(np.sum([x.memory_usage().sum() / 1e9 for x in \n",
    "[app, bureau, bureau_balance, cash, credit, previous, installments]]), 2)} gb.\"\"\")\n",
    "\n",
    "# Convert types to reduce memory usage\n",
    "app = convert_types(app)\n",
    "bureau = convert_types(bureau)\n",
    "bureau_balance = convert_types(bureau_balance)\n",
    "cash = convert_types(cash)\n",
    "credit = convert_types(credit)\n",
    "previous = convert_types(previous)\n",
    "installments = convert_types(installments)\n",
    "\n",
    "print(f\"\"\"Total memory after converting types: \\\n",
    "{round(np.sum([x.memory_usage().sum() / 1e9 for x in \n",
    "[app, bureau, bureau_balance, cash, credit, previous, installments]]), 2)} gb.\"\"\")\n",
    "\n",
    "# Set the index for locating\n",
    "for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "    dataset.set_index('SK_ID_CURR', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object memory usage.\n",
      "0.027462848 gb\n",
      "Category memory usage.\n",
      "0.015448612 gb\n",
      "Length of data:  1716428\n",
      "Number of unique categories:  15\n"
     ]
    }
   ],
   "source": [
    "print('Object memory usage.')\n",
    "print(bureau['CREDIT_TYPE'].astype('object').memory_usage() / 1e9, 'gb')\n",
    "\n",
    "print('Category memory usage.')\n",
    "print(bureau['CREDIT_TYPE'].astype('category').memory_usage() / 1e9, 'gb')\n",
    "\n",
    "print('Length of data: ', bureau.shape[0])\n",
    "print('Number of unique categories: ', bureau['CREDIT_TYPE'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Data\n",
    "\n",
    "Next, we partition the data into 104 separate datasets based on the client id, `SK_ID_CURR`. Each partition by itself can be used to make an `EntitySet` and later a feature matrix. One partition will contain seven data tables, each with only the data associated with the set clients. 104 partitions is sort of an arbitrary number and it might be worth exploring other options to see which works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(user_list, partition):\n",
    "    \"\"\"Creates a dataset with only the users in `user_list`.\"\"\"\n",
    "\n",
    "    \n",
    "    # Make the directory\n",
    "    directory = '../input/partitions/p%d' % (partition + 1)\n",
    "    if os.path.exists(directory):\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        # Subset based on user list\n",
    "        app_subset = app[app.index.isin(user_list)].copy().reset_index()\n",
    "        bureau_subset = bureau[bureau.index.isin(user_list)].copy().reset_index()\n",
    "\n",
    "        # Drop SK_ID_CURR from bureau_balance, cash, credit, and installments\n",
    "        bureau_balance_subset = bureau_balance[bureau_balance.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        cash_subset = cash[cash.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        credit_subset = credit[credit.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        previous_subset = previous[previous.index.isin(user_list)].copy().reset_index()\n",
    "        installments_subset = installments[installments.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        \n",
    "\n",
    "        # Save data to the directory\n",
    "        app_subset.to_csv('%s/app.csv' % directory, index = False)\n",
    "        bureau_subset.to_csv('%s/bureau.csv' % directory, index = False)\n",
    "        bureau_balance_subset.to_csv('%s/bureau_balance.csv' % directory, index = False)\n",
    "        cash_subset.to_csv('%s/cash.csv' % directory, index = False)\n",
    "        credit_subset.to_csv('%s/credit.csv' % directory, index = False)\n",
    "        previous_subset.to_csv('%s/previous.csv' % directory, index = False)\n",
    "        installments_subset.to_csv('%s/installments.csv' % directory, index = False)\n",
    "\n",
    "        if partition % 10 == 0:\n",
    "            print('Saved all files in partition {} to {}.'.format(partition + 1, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break into 104 chunks\n",
    "chunk_size = app.shape[0] // 103\n",
    "\n",
    "# Construct an id list\n",
    "id_list = [list(app.iloc[i:i+chunk_size].index) for i in range(0, app.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids in id_list:         356255.\n",
      "Total length of application data: 356255.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Sanity check that we have not missed any ids\n",
    "print('Number of ids in id_list:         {}.'.format(len(list(chain(*id_list)))))\n",
    "print('Total length of application data: {}.'.format(len(app)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all files in partition 1 to ../input/partitions/p1.\n",
      "Saved all files in partition 11 to ../input/partitions/p11.\n",
      "Saved all files in partition 21 to ../input/partitions/p21.\n",
      "Saved all files in partition 31 to ../input/partitions/p31.\n",
      "Saved all files in partition 41 to ../input/partitions/p41.\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "for i, ids in enumerate(id_list):\n",
    "    # Create a partition based on the ids\n",
    "    create_partition(ids, i)\n",
    "    \n",
    "end = timer()\n",
    "print(f'Partitioning took {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can independently generate the feature matrix for each partition because the partition contains all the data for that group of clients. These partitioned feature matrices can then be joined together into larger feature matrices, and eventually one single matrix with all of the clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Feature names\n",
    "\n",
    "We already calculated the feature names, so we can read them in. This avoids the need to have to recalculate the features on each partition. Instead of using `ft.dfs`, if we have the feature names, we can use `ft.calculate_feature_matrix` and pass in the `EntitySet` and the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurenames = ft.load_features('../input/feature_names.txt')\n",
    "print(len(featurenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature matrix, we'll make 1820 features! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Types\n",
    "\n",
    "If the Automated notebook, we specified the variable types when adding entities to the entityset. However, since we already properly defined the data types for each column, Featuretools will now infer the correct variable type. For example, while before we have Booleans mapped to integers which would be interpreted as numeric, now the Booleans are represented as Booleans and hence will be correctly inferred by Featuretools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_types = {'FLAG_CONT_MOBILE': vtypes.Boolean, 'FLAG_DOCUMENT_10': vtypes.Boolean, 'FLAG_DOCUMENT_11': vtypes.Boolean, 'FLAG_DOCUMENT_12': vtypes.Boolean, 'FLAG_DOCUMENT_13': vtypes.Boolean, 'FLAG_DOCUMENT_14': vtypes.Boolean, 'FLAG_DOCUMENT_15': vtypes.Boolean, 'FLAG_DOCUMENT_16': vtypes.Boolean, 'FLAG_DOCUMENT_17': vtypes.Boolean, 'FLAG_DOCUMENT_18': vtypes.Boolean, 'FLAG_DOCUMENT_19': vtypes.Boolean, 'FLAG_DOCUMENT_2': vtypes.Boolean, 'FLAG_DOCUMENT_20': vtypes.Boolean, 'FLAG_DOCUMENT_21': vtypes.Boolean, 'FLAG_DOCUMENT_3': vtypes.Boolean, 'FLAG_DOCUMENT_4': vtypes.Boolean, 'FLAG_DOCUMENT_5': vtypes.Boolean, 'FLAG_DOCUMENT_6': vtypes.Boolean, 'FLAG_DOCUMENT_7': vtypes.Boolean, 'FLAG_DOCUMENT_8': vtypes.Boolean, 'FLAG_DOCUMENT_9': vtypes.Boolean, 'FLAG_EMAIL': vtypes.Boolean, 'FLAG_EMP_PHONE': vtypes.Boolean, 'FLAG_MOBIL': vtypes.Boolean, 'FLAG_PHONE': vtypes.Boolean, 'FLAG_WORK_PHONE': vtypes.Boolean, 'LIVE_CITY_NOT_WORK_CITY': vtypes.Boolean, 'LIVE_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REG_CITY_NOT_LIVE_CITY': vtypes.Boolean, 'REG_CITY_NOT_WORK_CITY': vtypes.Boolean, 'REG_REGION_NOT_LIVE_REGION': vtypes.Boolean, 'REG_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REGION_RATING_CLIENT': vtypes.Ordinal, 'REGION_RATING_CLIENT_W_CITY': vtypes.Ordinal, 'HOUR_APPR_PROCESS_START': vtypes.Ordinal}\n",
    "# previous_types = {'NFLAG_LAST_APPL_IN_DAY': vtypes.Boolean, \n",
    "#              'NFLAG_INSURED_ON_APPROVAL': vtypes.Boolean}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Create EntitySet from Partition \n",
    "\n",
    "The next function takes a single partition of data and make an `EntitySet`. We won't save these entitysets to disk, but instead will use them in Dask. Therefore, if we want to make any changes to the `EntitySet`, such as adding in interesting values or seed features, we can alter this function and remake the `EntitySet` without having to rewrite all the Entity Sets on disk. Writing the entity sets to disk would be another option if we are sure that they won't ever change. For greater flexibility, we write the data partitions to disk (as done above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path.\"\"\"\n",
    "    \n",
    "    # Read in data\n",
    "    app = pd.read_csv('%s/app.csv' % path)\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % path)\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % path)\n",
    "    previous = pd.read_csv('%s/previous.csv' % path)\n",
    "    credit = pd.read_csv('%s/credit.csv' % path)\n",
    "    installments = pd.read_csv('%s/installments.csv' % path)\n",
    "    cash = pd.read_csv('%s/cash.csv' % path)\n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    # Entities with a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR',\n",
    "                                  variable_types = app_types)\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV',\n",
    "                                  variable_types = previous_types)\n",
    "\n",
    "    # Entities that do not have a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                  make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                                  make_index = True, index = 'cash_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                                  make_index = True, index = 'installments_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                                  make_index = True, index = 'credit_index')\n",
    "    \n",
    "    # Relationship between app_train and bureau\n",
    "    r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationship between bureau and bureau balance\n",
    "    r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "    # Relationship between current app and previous apps\n",
    "    r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationships between previous apps and cash, installments, and credit\n",
    "    r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "    r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "    r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "    \n",
    "    # Add in the defined relationships\n",
    "    es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                               r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function to make sure it can make an `EntitySet` from a data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es1 = entityset_from_partition('../input/partitions/p1')\n",
    "es1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function looks like it works as intended. The next step is to write a function that can take a single `EntitySet` and the `features` we want to build, and make a feature matrix. This is simple using `ft.calculate_feature_matrix`. \n",
    "\n",
    "# Function to Create Feature Matrix from EntitySet \n",
    "\n",
    "With the entity set and the feature names, generating the feature matrix is a one-liner in Featuretools. Since we are going to use Dask for parallelizing the operation, we'll set the number of jobs to 1. The `chunk_size` is an extremely important parameter, and I'd suggest experimenting with this to find the optimal value. Since we aren't getting any updates, it might make sense to set the chunk size as large as possible. We can try setting it to the length of the `app` dataframe for each entityset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es, feature_names):\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature names\"\"\"\n",
    "\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_names, \n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = es['app'].df.shape[0])\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we test the function using the entityset from the first partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm1 = feature_matrix_from_entityset(es1, featurenames)\n",
    "fm1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the parts needed to create our feature matrixes. The last step is to get Dask to run this in parallel. For the dask implementation, we'll set `n_jobs = 1` and `verbose = 0` because Dask already runs the calculation in parallel and because Dask should have a progress bar that we can view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "We will use the Dask utility `delayed` to parallelize the operation. We iterate through each path in a list of the partitions and tell dask to first create the entity set from the partition, then create the featurematrix from the entityset, and finally, append the feature matrix to a list of feature matrices. The last step is to `concat`enate all of the feature matrices together to get one final matrix that we save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(processes = True)\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/partitions/p100',\n",
       " '../input/partitions/p4',\n",
       " '../input/partitions/p3',\n",
       " '../input/partitions/p101',\n",
       " '../input/partitions/p2',\n",
       " '../input/partitions/p5',\n",
       " '../input/partitions/p19',\n",
       " '../input/partitions/p26']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['../input/partitions/%s' % file for file in os.listdir('../input/partitions/')]\n",
    "paths[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature matrix 0\n",
      "Feature Matrix 0 complete, Time Elapsed: 838.17 seconds.\n",
      "Starting feature matrix 1\n",
      "Feature Matrix 1 complete, Time Elapsed: 993.07 seconds.\n",
      "Starting feature matrix 2\n",
      "Feature Matrix 2 complete, Time Elapsed: 975.8 seconds.\n",
      "Starting feature matrix 3\n",
      "Feature Matrix 3 complete, Time Elapsed: 1022.86 seconds.\n",
      "Starting feature matrix 4\n",
      "Feature Matrix 4 complete, Time Elapsed: 1009.8 seconds.\n",
      "Starting feature matrix 5\n",
      "Feature Matrix 5 complete, Time Elapsed: 954.63 seconds.\n",
      "Starting feature matrix 6\n",
      "Feature Matrix 6 complete, Time Elapsed: 895.52 seconds.\n",
      "Starting feature matrix 7\n",
      "Feature Matrix 7 complete, Time Elapsed: 904.09 seconds.\n",
      "Starting feature matrix 8\n",
      "Feature Matrix 8 complete, Time Elapsed: 903.8 seconds.\n",
      "Starting feature matrix 9\n",
      "Feature Matrix 9 complete, Time Elapsed: 905.32 seconds.\n",
      "Starting feature matrix 10\n",
      "Feature Matrix 10 complete, Time Elapsed: 933.32 seconds.\n",
      "Starting feature matrix 11\n",
      "Feature Matrix 11 complete, Time Elapsed: 929.27 seconds.\n",
      "Starting feature matrix 12\n",
      "Feature Matrix 12 complete, Time Elapsed: 526.19 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_index = 1\n",
    "overall_start = timer()\n",
    "\n",
    "# Iterate through 8 paths at a time\n",
    "for i, end_index in enumerate(range(9, len(paths) + 5, 8)):\n",
    "    \n",
    "    # Subset to the 8 paths\n",
    "    if end_index > len(paths):\n",
    "        subset_paths = paths[start_index:]\n",
    "    else:\n",
    "        subset_paths = paths[start_index: end_index]\n",
    "    \n",
    "    # Empty list of feature matrices\n",
    "    fms = []\n",
    "\n",
    "    # Iterate through the paths\n",
    "    for path in subset_paths:\n",
    "\n",
    "        # Make the entityset\n",
    "        es = delayed(entityset_from_partition)(path)\n",
    "\n",
    "        # Make the feature matrix and add to the list\n",
    "        fm = delayed(feature_matrix_from_entityset)(es, feature_names = featurenames)\n",
    "        fms.append(fm)\n",
    "\n",
    "    # Final operation will be to concatenate together all of the feature matrices\n",
    "    X = delayed(pd.concat)(fms, axis = 0)\n",
    "    \n",
    "    print(f\"Starting feature matrix {i}\")\n",
    "    start = timer()\n",
    "    feature_matrix = X.compute()\n",
    "    end = timer()\n",
    "    \n",
    "    print(f\"Feature Matrix {i} complete, Time Elapsed: {round(end - start, 2)} seconds.\")\n",
    "    \n",
    "    # Save the feature matrix to disk\n",
    "    feature_matrix.to_csv('../input/fm/%s.csv' % i, index = True)\n",
    "    \n",
    "    # Start index becomes previous ending index\n",
    "    start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/fm/6.csv',\n",
       " '../input/fm/7.csv',\n",
       " '../input/fm/5.csv',\n",
       " '../input/fm/4.csv',\n",
       " '../input/fm/0.csv',\n",
       " '../input/fm/1.csv',\n",
       " '../input/fm/3.csv',\n",
       " '../input/fm/2.csv',\n",
       " '../input/fm/10.csv',\n",
       " '../input/fm/11.csv',\n",
       " '../input/fm/12.csv',\n",
       " '../input/fm/9.csv',\n",
       " '../input/fm/8.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base directory for feature matrices\n",
    "base = '../input/fm/'\n",
    "fm_paths = [base + p for p in os.listdir(base) if '.csv' in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elasped: 97.33 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Start a new client with processes\n",
    "client = Client(processes = False)\n",
    "\n",
    "# Empty list for feature matrices\n",
    "fms = []\n",
    "\n",
    "# Iterate through the feature matrices\n",
    "for path in fm_paths:\n",
    "    # Read in each dataframe and append to list\n",
    "    X = delayed(pd.read_csv)(path, index_col = 0)\n",
    "    fms.append(X)\n",
    "\n",
    "# Concatenate all the matrices together (append rows)\n",
    "fm_out = delayed(pd.concat)(fms, axis = 0)\n",
    "\n",
    "# Time how long operate takes\n",
    "start = timer()\n",
    "feature_matrix = fm_out.compute()\n",
    "end = timer()\n",
    "\n",
    "print(f'Time elasped: {round(end - start, 2)} seconds.')\n",
    "overall_end = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352693, 1820)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature matrix is exactly the expected shape: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time for Feature Matrix Calculation: -12574.52.\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Time for Feature Matrix Calculation: {round(overall_start - overall_end, 2)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "\n",
    "# # Context for progress and profiling\n",
    "# with ProgressBar(), Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "#     feature_matrix = X.compute()\n",
    "\n",
    "# end = timer()\n",
    "\n",
    "# print(f'Total time elapsed: {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature matrix\n",
    "# feature_matrix.to_csv('../input/feature_matrix.csv', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations of Run\n",
    "\n",
    "We can use Bokeh and the built in plotting capabilities of Dask to plot the resources used during the computation. This is not necessary for the project but might be interesting for understanding how Dask operates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/prof.txt', 'w') as f:\n",
    "    f.write(str(prof.results))\n",
    "    \n",
    "with open('../input/rprof.txt', 'w') as f:\n",
    "    f.write(str(rprof.results))\n",
    "    \n",
    "with open('../input/cprof.txt', 'w') as f:\n",
    "    f.write(str(cprof.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we used Dask to complete an operation that normally would have taken far more resources than we have available on our personal computer. Although featuretools supports parallel processing, there are still some issues that need to be worked out, and we can take matters into our own hands to get the job done! This will allow anyone with reasonable hardware to take advantage of featuretools! The next notebook is Sampling and Feature Selection where we limit reduce the numbers of features to allow for reasonable modeling times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
