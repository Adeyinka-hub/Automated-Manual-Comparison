{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools Implementation with Dask\n",
    "\n",
    "In this notebook we will use Dask to run deep feature synthesis on the entire dataset and generate the feature matrix. This operation is not feasible with a personal laptop on the entire Kaggle Home Credit dataset at once, but using Dask we can run the operations in parallel and complete this operation on a laptop in a reasonable time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Data\n",
    "\n",
    "First, we partition the data into 100 separate datasets based on the client id, `SK_ID_CURR`. These partitions can then be used in combination with Dask to take advantage of all the resources on our machine for generating the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets and replace the anomalous values\n",
    "app_train = pd.read_csv('../input/application_train.csv').replace({365243: np.nan})\n",
    "app_test = pd.read_csv('../input/application_test.csv').replace({365243: np.nan})\n",
    "bureau = pd.read_csv('../input/bureau.csv').replace({365243: np.nan})\n",
    "bureau_balance = pd.read_csv('../input/bureau_balance.csv').replace({365243: np.nan})\n",
    "cash = pd.read_csv('../input/POS_CASH_balance.csv').replace({365243: np.nan})\n",
    "credit = pd.read_csv('../input/credit_card_balance.csv').replace({365243: np.nan})\n",
    "previous = pd.read_csv('../input/previous_application.csv').replace({365243: np.nan})\n",
    "installments = pd.read_csv('../input/installments_payments.csv').replace({365243: np.nan})\n",
    "\n",
    "app_test['TARGET'] = np.nan\n",
    "\n",
    "# Join together training and testing\n",
    "app = app_train.append(app_test, ignore_index = True, sort = True)\n",
    "\n",
    "# All ids should be integers\n",
    "for index in ['SK_ID_CURR', 'SK_ID_PREV', 'SK_ID_BUREAU']:\n",
    "    for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "        if index in list(dataset.columns):\n",
    "            # Convert to integers after filling in missing values (not sure why values are missing)\n",
    "            dataset[index] = dataset[index].fillna(0).astype(np.int64)\n",
    "\n",
    "# Need `SK_ID_CURR` in every dataset\n",
    "bureau_balance = bureau_balance.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], \n",
    "                                      on = 'SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "\n",
    "# Set the index for locating\n",
    "for dataset in [app, bureau, bureau_balance, cash, credit, previous, installments]:\n",
    "    dataset.set_index('SK_ID_CURR', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(user_list, partition):\n",
    "    \"\"\"Creates an entityset with only the users in `user_list`. \n",
    "       Main purpose is partioning data\"\"\"\n",
    "    \n",
    "    # Make the directory\n",
    "    directory = '../input/partitions/p%d' % (partition + 1)\n",
    "    if os.path.exists(directory):\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "        # Subset based on user list\n",
    "        app_subset = app[app.index.isin(user_list)].copy().reset_index()\n",
    "        bureau_subset = bureau[bureau.index.isin(user_list)].copy().reset_index()\n",
    "\n",
    "        # Drop SK_ID_CURR from bureau_balance, cash, credit, and installments\n",
    "        bureau_balance_subset = bureau_balance[bureau_balance.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        cash_subset = cash[cash.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        credit_subset = credit[credit.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        previous_subset = previous[previous.index.isin(user_list)].copy().reset_index()\n",
    "        installments_subset = installments[installments.index.isin(user_list)].copy().reset_index(drop = True)\n",
    "        \n",
    "\n",
    "        # Save data to the directory\n",
    "        app_subset.to_csv('%s/app.csv' % directory, index = False)\n",
    "        bureau_subset.to_csv('%s/bureau.csv' % directory, index = False)\n",
    "        bureau_balance_subset.to_csv('%s/bureau_balance.csv' % directory, index = False)\n",
    "        cash_subset.to_csv('%s/cash.csv' % directory, index = False)\n",
    "        credit_subset.to_csv('%s/credit.csv' % directory, index = False)\n",
    "        previous_subset.to_csv('%s/previous.csv' % directory, index = False)\n",
    "        installments_subset.to_csv('%s/installments.csv' % directory, index = False)\n",
    "\n",
    "        print('Saved all files in partition {} to {}.'.format(partition + 1, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break into 100 chunks\n",
    "chunk_size = app.shape[0] // 100\n",
    "\n",
    "# Construct an id list\n",
    "id_list = [list(app.iloc[i:i+chunk_size].index) for i in range(0, app.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Sanity check that we have not missed any ids\n",
    "print('Number of ids in id_list:         {}.'.format(len(list(chain(*id_list)))))\n",
    "print('Total length of application data: {}.'.format(len(app)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ids in enumerate(id_list):\n",
    "    # Create a partition based on the ids\n",
    "    create_partition(ids, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from Partitioning Data\n",
    "\n",
    "Now that the data has been partitioned into 100 sections, we can use Dask to parallize computing the feature matrix. We can independently generate the feature matrix for each partition because the partition contains all the data for that group of clients. These partitioned feature matrices can then be joined together into one large feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Feature names\n",
    "\n",
    "We already calculated the feature names, so it's simple to read them in. This avoids the need to have to recalculate the features on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "featurenames = ft.load_features('../input/feature_names.txt')\n",
    "print(len(featurenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Types\n",
    "\n",
    "Following are the variable types defined in the Automated Feature Engineering notebook. Defining them here prevents us from needing to define them for every entity separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_types = {'FLAG_CONT_MOBILE': vtypes.Boolean, 'FLAG_DOCUMENT_10': vtypes.Boolean, 'FLAG_DOCUMENT_11': vtypes.Boolean, 'FLAG_DOCUMENT_12': vtypes.Boolean, 'FLAG_DOCUMENT_13': vtypes.Boolean, 'FLAG_DOCUMENT_14': vtypes.Boolean, 'FLAG_DOCUMENT_15': vtypes.Boolean, 'FLAG_DOCUMENT_16': vtypes.Boolean, 'FLAG_DOCUMENT_17': vtypes.Boolean, 'FLAG_DOCUMENT_18': vtypes.Boolean, 'FLAG_DOCUMENT_19': vtypes.Boolean, 'FLAG_DOCUMENT_2': vtypes.Boolean, 'FLAG_DOCUMENT_20': vtypes.Boolean, 'FLAG_DOCUMENT_21': vtypes.Boolean, 'FLAG_DOCUMENT_3': vtypes.Boolean, 'FLAG_DOCUMENT_4': vtypes.Boolean, 'FLAG_DOCUMENT_5': vtypes.Boolean, 'FLAG_DOCUMENT_6': vtypes.Boolean, 'FLAG_DOCUMENT_7': vtypes.Boolean, 'FLAG_DOCUMENT_8': vtypes.Boolean, 'FLAG_DOCUMENT_9': vtypes.Boolean, 'FLAG_EMAIL': vtypes.Boolean, 'FLAG_EMP_PHONE': vtypes.Boolean, 'FLAG_MOBIL': vtypes.Boolean, 'FLAG_PHONE': vtypes.Boolean, 'FLAG_WORK_PHONE': vtypes.Boolean, 'LIVE_CITY_NOT_WORK_CITY': vtypes.Boolean, 'LIVE_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REG_CITY_NOT_LIVE_CITY': vtypes.Boolean, 'REG_CITY_NOT_WORK_CITY': vtypes.Boolean, 'REG_REGION_NOT_LIVE_REGION': vtypes.Boolean, 'REG_REGION_NOT_WORK_REGION': vtypes.Boolean, 'REGION_RATING_CLIENT': vtypes.Ordinal, 'REGION_RATING_CLIENT_W_CITY': vtypes.Ordinal, 'HOUR_APPR_PROCESS_START': vtypes.Ordinal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_types = {'NFLAG_LAST_APPL_IN_DAY': vtypes.Boolean, \n",
    "             'NFLAG_INSURED_ON_APPROVAL': vtypes.Boolean}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Create EntitySet from Partition \n",
    "\n",
    "The data has aleady been broken into 50 partitions so we will write a function that takes a single partition and creates the entity set for that data. This can then be passed into a function that calculates the feature matrix from the entityset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entityset_from_partition(path):\n",
    "    \"\"\"Create an EntitySet from a partition of data specified as a path\"\"\"\n",
    "    \n",
    "    app = pd.read_csv('%s/app.csv' % path)\n",
    "    bureau = pd.read_csv('%s/bureau.csv' % path)\n",
    "    bureau_balance = pd.read_csv('%s/bureau_balance.csv' % path)\n",
    "    previous = pd.read_csv('%s/previous.csv' % path)\n",
    "    credit = pd.read_csv('%s/credit.csv' % path)\n",
    "    installments = pd.read_csv('%s/installments.csv' % path)\n",
    "    cash = pd.read_csv('%s/cash.csv' % path)\n",
    "    \n",
    "    # Empty entityset\n",
    "    es = ft.EntitySet(id = 'clients')\n",
    "    \n",
    "    # Entities with a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR',\n",
    "                                  variable_types = app_types)\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV',\n",
    "                                  variable_types = previous_types)\n",
    "\n",
    "    # Entities that do not have a unique index\n",
    "    es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                                  make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                                  make_index = True, index = 'cash_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                                  make_index = True, index = 'installments_index')\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                                  make_index = True, index = 'credit_index')\n",
    "    \n",
    "    # Relationship between app_train and bureau\n",
    "    r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationship between bureau and bureau balance\n",
    "    r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "    # Relationship between current app and previous apps\n",
    "    r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "    # Relationships between previous apps and cash, installments, and credit\n",
    "    r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "    r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "    r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "    \n",
    "    # Add in the defined relationships\n",
    "    es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                               r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "\n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function to make sure it can make an `EntitySet` from a data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: clients\n",
       "  Entities:\n",
       "    app [Rows: 3562, Columns: 122]\n",
       "    bureau [Rows: 16629, Columns: 17]\n",
       "    previous [Rows: 16670, Columns: 37]\n",
       "    bureau_balance [Rows: 172429, Columns: 4]\n",
       "    cash [Rows: 99505, Columns: 8]\n",
       "    installments [Rows: 133182, Columns: 8]\n",
       "    credit [Rows: 36921, Columns: 23]\n",
       "  Relationships:\n",
       "    bureau.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU\n",
       "    previous.SK_ID_CURR -> app.SK_ID_CURR\n",
       "    cash.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    installments.SK_ID_PREV -> previous.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> previous.SK_ID_PREV"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es1 = entityset_from_partition('../input/partitions/p1')\n",
    "es1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works! The next step is to create a feature matrix for the partition from the `EntitySet` and the `features`. \n",
    "\n",
    "## Function to Create Featurematrix from EntitySet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix_from_entityset(es, feature_names):\n",
    "    \"\"\"Run deep feature synthesis from an entityset and feature names\"\"\"\n",
    "\n",
    "    feature_matrix = ft.calculate_feature_matrix(feature_names, \n",
    "                                                 entityset=es, \n",
    "                                                 n_jobs = 1, \n",
    "                                                 verbose = 0,\n",
    "                                                 chunk_size = 1000)\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm1 = feature_matrix_from_entityset(es1, featurenames)\n",
    "fm1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the parts needed to create our feature matrixes. The last step is to get Dask to run this in parallel. For the dask implementation, we'll set `n_jobs = 1` and `verbose = 0` because Dask already runs the calculation in parallel and because Dask should have a progress bar that we can view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "We will use the Dask utility `delayed` to parallelize the operation. We iterate through each path in a list of the partitions and tell dask to first create the entity set from the partition, then create the featurematrix from the entityset, and finally, append the feature matrix to a list of feature matrices. The last step is to `concat`enate all of the feature matrices together to get one final matrix that we save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(processes = True)\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/partitions/p100',\n",
       " '../input/partitions/p4',\n",
       " '../input/partitions/p3',\n",
       " '../input/partitions/p101',\n",
       " '../input/partitions/p2',\n",
       " '../input/partitions/p5',\n",
       " '../input/partitions/p19',\n",
       " '../input/partitions/p26']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = ['../input/partitions/%s' % file for file in os.listdir('../input/partitions/')]\n",
    "paths[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature matrix 0\n"
     ]
    }
   ],
   "source": [
    "start_index = 1\n",
    "overall_start = timer()\n",
    "\n",
    "# Iterate through 8 paths at a time\n",
    "for i, end_index in enumerate(range(9, len(paths) + 5, 8)):\n",
    "    \n",
    "    # Subset to the 8 paths\n",
    "    if end_index > len(paths):\n",
    "        subset_paths = paths[start_index:]\n",
    "    else:\n",
    "        subset_paths = paths[start_index: end_index]\n",
    "    \n",
    "    # Empty list of feature matrices\n",
    "    fms = []\n",
    "\n",
    "    # Iterate through the paths\n",
    "    for path in subset_paths:\n",
    "\n",
    "        # Make the entityset\n",
    "        es = delayed(entityset_from_partition)(path)\n",
    "\n",
    "        # Make the feature matrix\n",
    "        fm = delayed(feature_matrix_from_entityset)(es, feature_names = featurenames)\n",
    "        fms.append(fm)\n",
    "\n",
    "    # Final operation will be to concatenate together all of the feature matrices\n",
    "    X = delayed(pd.concat)(fms, axis = 0)\n",
    "    \n",
    "    print(f\"Starting feature matrix {i}\")\n",
    "    start = timer()\n",
    "    feature_matrix = X.compute()\n",
    "    end = timer()\n",
    "    \n",
    "    print(f\"Feature Matrix {i} complete, Time Elapsed: {round(end - start, 2)} seconds.\")\n",
    "    \n",
    "    # Save the feature matrix to disk\n",
    "    feature_matrix.to_csv('../input/fm/%s.csv' % i, index = True)\n",
    "    \n",
    "    # Start index becomes previous ending index\n",
    "    start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '../input/fm/'\n",
    "fm_paths = [base + p for p in os.listdir(base) if '.csv' in p]\n",
    "fm_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes = False)\n",
    "\n",
    "fms = []\n",
    "for path in fm_paths:\n",
    "    X = delayed(pd.read_csv)(path, index_col = 0)\n",
    "    fms.append(X)\n",
    "\n",
    "fm_out = delayed(pd.concat)(fms, axis = 0)\n",
    "\n",
    "start = timer()\n",
    "feature_matrix = fm_out.compute()\n",
    "end = timer()\n",
    "\n",
    "print(f'Time elasped: {round(end - start, 2)} seconds.')\n",
    "overall_end = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total Time for Feature Matrix Calculation: {round(overall_start - overall_end, 2)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timer()\n",
    "\n",
    "# # Context for progress and profiling\n",
    "# with ProgressBar(), Profiler() as prof, ResourceProfiler(dt=0.25) as rprof, CacheProfiler() as cprof:\n",
    "#     feature_matrix = X.compute()\n",
    "\n",
    "# end = timer()\n",
    "\n",
    "# print(f'Total time elapsed: {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature matrix\n",
    "# feature_matrix.to_csv('../input/feature_matrix.csv', chunksize = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations of Run\n",
    "\n",
    "We can use Bokeh and the built in plotting capabilities of Dask to plot the resources used during the computation. This is not necessary for the project but might be interesting for understanding how Dask operates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/prof.txt', 'w') as f:\n",
    "    f.write(str(prof.results))\n",
    "    \n",
    "with open('../input/rprof.txt', 'w') as f:\n",
    "    f.write(str(rprof.results))\n",
    "    \n",
    "with open('../input/cprof.txt', 'w') as f:\n",
    "    f.write(str(cprof.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we used Dask to complete an operation that normally would have taken far more resources than we have available on our personal computer. Although featuretools supports parallel processing, there are still some issues that need to be worked out, and we can take matters into our own hands to get the job done! This will allow anyone with reasonable hardware to take advantage of featuretools! The next notebook is Sampling and Feature Selection where we limit reduce the numbers of features to allow for reasonable modeling times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
