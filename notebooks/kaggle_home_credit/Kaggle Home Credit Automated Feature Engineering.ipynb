{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../images/featuretools.png)\n",
    "\n",
    "# Automated Feature Engineering on Home Credit Default Risk Data\n",
    "\n",
    "In this notebook, we will apply automated feature engineering using featuretools to the Home Credit Default Risk competition dataset. This problem is a machine learnign competition currently running on Kaggle where the objective is to predict if an applicant will default on a loan given historical information on past loans. The data is spread across seven different tables which makes this an ideal problem for automated feature engineering: all of the data must be gathered into a single dataframe for training (and one for testing) with the aim of capturing as much usable information for the prediction problem as possible. \n",
    "\n",
    "Our focus will be on using featuretools as efficiently as possible for a first pass on the problem. That means we will not spend much time maximizing the potential of the featuretools library, but will instead focus on getting a usable solution as quickly as possible. \n",
    "\n",
    "## Approach \n",
    "\n",
    "Automated feature engineering is not meant to replace the data scientist, but rather augment her work. While featuretools allows us to create thousands of feature requiring no domain knowledge whatsover, it also can magnify our domain knowledge by _building features on top of existing domain knowledge features_. Therefore, if build features by hand that are useful for the problem, we can potentially make these even more valuable by stacking additional features on top of these. In this notebook, we will include several of the features that we constructed in the traditional manual feature engineering section of the Manual Feature Engineering notebook as seed features. We will get both the domain knowledge encoded in this features as well as the efficient creation of thousands of features from featuretools. \n",
    "\n",
    "## Problem and Dataset\n",
    "\n",
    "The [Home Credit Default Risk competition](https://www.kaggle.com/c/home-credit-default-risk) currently running on Kaggle is a supervised classification task where the objective is to predict whether or not an applicant for a loan (known as a client) will default on the loan. The data comprises socio-economic indicators for the clients, loan specific financial information, and comprehensive data on previous loans at Home Credit (the institution sponsoring the competition) and other credit agencies. The metric for this competition is Receiver Operating Characteristic Area Under the Curve (ROC AUC) with predictions made in terms of the probability of default. We can evaluate our submissions both through cross-validation on the training data (for which we have the labels) or by submitting our test predictions to Kaggle to see where we place on the public leaderboard (which is calculated with only 10% of the testing data). \n",
    "\n",
    "The Home Credit Default Risk dataset ([available for download here](https://www.kaggle.com/c/home-credit-default-risk/data)) consists of seven tables of data:\n",
    "\n",
    "* application_train/application_test: the main training/testing data for each client at Home Credit. The information includes both socioeconomic indicators for the client and loan-specific characteristics. Each loan has its own row and is uniquely identified by the feature `SK_ID_CURR`. The training application data comes with the `TARGET` indicating 0: the loan was repaid or 1: the loan was not repaid. \n",
    "* bureau: data concerning client's previous credits from other financial institutions (not Home Credit). Each previous credit has its own row in bureau, but one client in the application data can have multiple previous credits. The previous credits are uniquely identified by the feature `SK_ID_BUREAU`.\n",
    "* bureau_balance: monthly balance data about the credits in bureau. Each row has information for one month about a previous credit and a single previous credit can have multiple rows. This is linked backed to the bureau loan data by `SK_ID_BUREAU` (not unique in this dataframe).\n",
    "* previous_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each client in the application data can have multiple previous loans. Each previous application has one row in this dataframe and is uniquely identified by the feature `SK_ID_PREV`. \n",
    "* POS_CASH_BALANCE: monthly data about previous point of sale or cash loans from the previous loan data. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows. This is linked backed to the previous loan data by `SK_ID_PREV` (not unique in this dataframe).\n",
    "* credit_card_balance: monthly data about previous credit cards loans from the previous loan data. Each row is one month of a credit card balance, and a single credit card can have many rows. This is linked backed to the previous loan data by `SK_ID_PREV` (not unique in this dataframe).\n",
    "* installments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment. This is linked backed to the previous loan data by `SK_ID_PREV` (not unique in this dataframe).\n",
    "\n",
    "The image below shows the seven tables and the variables linking them:\n",
    "\n",
    "![](../../images/kaggle_home_credit/home_credit_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "\n",
    "# matplotlit and seaborn for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 22\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the datasets \n",
    "app_train = pd.read_csv('../../data/kaggle_home_credit/application_train.csv').replace({365243: np.nan})\n",
    "app_test = pd.read_csv('../../data/kaggle_home_credit/application_test.csv').replace({365243: np.nan})\n",
    "bureau = pd.read_csv('../../data/kaggle_home_credit/bureau.csv').replace({365243: np.nan})\n",
    "bureau_balance = pd.read_csv('../../data/kaggle_home_credit/bureau_balance.csv').replace({365243: np.nan})\n",
    "cash = pd.read_csv('../../data/kaggle_home_credit/POS_CASH_balance.csv').replace({365243: np.nan})\n",
    "credit = pd.read_csv('../../data/kaggle_home_credit/credit_card_balance.csv').replace({365243: np.nan})\n",
    "previous = pd.read_csv('../../data/kaggle_home_credit/previous_application.csv').replace({365243: np.nan})\n",
    "installments = pd.read_csv('../../data/kaggle_home_credit/installments_payments.csv').replace({365243: np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll join the train and test set together but add a separate column identifying the set. This is important because we are going to want to apply the same exact procedures to each dataset. It's safest to just join them together and treat them as a single dataframe. Later we can separate out the training and testing data using `train = app[app[\"TARGET\"].notnull()]` and `test = app[app[\"TARGET\"].isnull()]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add identifying column\n",
    "app_train['set'] = 'train'\n",
    "app_test['set'] = 'test'\n",
    "app_test[\"TARGET\"] = np.nan\n",
    "\n",
    "# Append the dataframes\n",
    "app = app_train.append(app_test, ignore_index = True, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# Append the dataframes\n",
    "app = app_train.append(app_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools Basics\n",
    "\n",
    "[Featuretools](https://docs.featuretools.com/#minute-quick-start) is an open-source Python library for automatically creating features out of a set of related tables using a technique called [deep feature synthesis](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf). Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.\n",
    "\n",
    "There are a few concepts that we will cover along the way:\n",
    "\n",
    "* [Entities and EntitySets](https://docs.featuretools.com/loading_data/using_entitysets.html)\n",
    "* [Relationships between tables](https://docs.featuretools.com/loading_data/using_entitysets.html#adding-a-relationship)\n",
    "* [Feature primitives](https://docs.featuretools.com/automated_feature_engineering/primitives.html): aggregations and transformations\n",
    "* [Deep feature synthesis](https://docs.featuretools.com/automated_feature_engineering/afe.html)\n",
    "\n",
    "# Entities and Entitysets\n",
    "\n",
    "An entity is simply a table or in Pandas, a `dataframe`. The observations are in the rows and the features in the columns. An entity in featuretools must have a unique index where none of the elements are duplicated.  Currently, only `app`, `bureau`, and `previous` have unique indices (`SK_ID_CURR`, `SK_ID_BUREAU`, and `SK_ID_PREV` respectively). For the other dataframes, we must pass in `make_index = True` and then specify the name of the index. Entities can also have time indices where each entry is identified by a unique time. (There are not datetimes in any of the data, but there are relative times, given in months or days, that we could consider treating as time variables).\n",
    "\n",
    "An [EntitySet](https://docs.featuretools.com/loading_data/using_entitysets.html) is a collection of tables and the relationships between them. This can be thought of a data structute with its own methods and attributes. Using an EntitySet allows us to group together multiple tables and manipulate them much quicker than individual tables. \n",
    "\n",
    "First we'll make an empty entityset named clients to keep track of all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity set with id applications\n",
    "es = ft.EntitySet(id = 'clients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Types\n",
    "\n",
    "Featuretools will automatically infer the variable types. However, there may be some cases where we need to explicitly tell featuretools the variable type such as when a boolean variable is represented as an integer. Variable types in featuretools must be specified as a dictionary. \n",
    "\n",
    "We will first work with the `app` data to specify the proper variable types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools.variable_types as vtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_types = {}\n",
    "\n",
    "# Handle the Boolean variables:\n",
    "for col in app:\n",
    "    if app[col].nunique() == 2:\n",
    "        app_types[col] = vtypes.Boolean\n",
    "\n",
    "print('There are {} Boolean variables.'.format(len(app_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_types['REGION_RATING_CLIENT'] = vtypes.Ordinal\n",
    "app_types['REGION_RATING_CLIENT_W_CITY'] = vtypes.Ordinal\n",
    "app_types['WEEKDAY_APPR_PROCESS_START'] = vtypes.Ordinal\n",
    "app_types['HOUR_APPR_PROCESS_START'] = vtypes.Ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the other data tables to see if there are any columns that should be recorded as Boolean. We also need to read through the column descriptions to determine if there are any other variables types (such as ordinal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.any(bureau.nunique() == 2))\n",
    "print(np.any(bureau_balance.nunique == 2))\n",
    "print(np.any(credit.nunique() == 2))\n",
    "print(np.any(cash.nunique() == 2))\n",
    "print(np.any(previous.nunique() == 2))\n",
    "print(np.any(installments.nunique() == 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_types = {}\n",
    "\n",
    "# Handle the Boolean variables:\n",
    "for col in previous:\n",
    "    if previous[col].nunique() == 2:\n",
    "        previous_types[col] = vtypes.Boolean\n",
    "\n",
    "print('There are {} Boolean variables.'.format(len(previous_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `credit`, `cash`, and `installments` data all have the `SK_ID_CURR` key. However, we do not actually need this variable in these dataframes because we link them to `app` through the `previous` dataframe with the `SK_ID_PREV` variable. (We can't directly add the relationships using `SK_ID_CURR` and using the `previous` dataframe because this would create a diamond graph.) We don't want to make features from `SK_ID_CURR` since it is an arbitrary id and should have no predictive power. These features are irrelevant and hence would only slow down model training and perhaps lead to poorer performance. \n",
    "\n",
    "Our options to handle these is either to tell featuretools to ignore them, or to drop the features before including them in the entityset. We will take these latter approach since it alleviates the need to remember to tell featuretools to ignore these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments = installments.drop(columns = ['SK_ID_CURR'])\n",
    "credit = credit.drop(columns = ['SK_ID_CURR'])\n",
    "cash = cash.drop(columns = ['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in Domain Features to Base Data\n",
    "\n",
    "In the traditional manual feature engineering notebook, we created a number of new features by manipulating columns of the `app` dataframe itself. Since these are in the base dataframe, featuretools cannot build features on top of these, but since these features are useful, we should still include them in the `app` data. Later we will see how we can use seed features to allow featuretools to build on top of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app['LOAN_RATE'] = app['AMT_ANNUITY'] / app['AMT_CREDIT'] \n",
    "app['CREDIT_INCOME_RATIO'] = app['AMT_CREDIT'] / app['AMT_INCOME_TOTAL']\n",
    "app['EMPLOYED_BIRTH_RATIO'] = app['DAYS_EMPLOYED'] / app['DAYS_BIRTH']\n",
    "\n",
    "app['EXT_SOURCE_SUM'] = app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].sum(axis = 1)\n",
    "app['EXT_SOURCE_MEAN'] = app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis = 1)\n",
    "app['AMT_REQ_SUM'] = app[[x for x in app.columns if 'AMT_REQ_' in x]].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Entities\n",
    "\n",
    "Now we define each entity, or table of data. We need to pass in an index if the data has one or `make_index = True` if not. In the cases where we need to make an index, we must supply a name for the index. We also need to pass in the variable types that we identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities with a unique index\n",
    "es = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR',\n",
    "                              variable_types = app_types)\n",
    "\n",
    "es = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n",
    "\n",
    "es = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV',\n",
    "                              variable_types = previous_types)\n",
    "\n",
    "# Entities that do not have a unique index\n",
    "es = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n",
    "                              make_index = True, index = 'bureaubalance_index')\n",
    "\n",
    "es = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n",
    "                              make_index = True, index = 'cash_index')\n",
    "\n",
    "es = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n",
    "                              make_index = True, index = 'installments_index')\n",
    "\n",
    "es = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n",
    "                              make_index = True, index = 'credit_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationships\n",
    "\n",
    "Relationships are a fundamental concept not only in featuretools, but in any relational database. The best way to think of a one-to-many relationship is with the analogy of parent-to-child. A parent is a single individual, but can have mutliple children. The children can then have multiple children of their own. In a _parent table_, each individual has a single row. Each individual in the parent table can have multiple rows in the _child table_. \n",
    "\n",
    "As an example, the `app` dataframe has one row for each client  (`SK_ID_CURR`) while the `bureau` dataframe has multiple previous loans (`SK_ID_PREV`) for each parent (`SK_ID_CURR`). Therefore, the `bureau` dataframe is the child of the `app` dataframe. The `bureau` dataframe in turn is the parent of `bureau_balance` because each loan has one row in `bureau` but multiple monthly records in `bureau_balance`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parent: app, Parent Variable: SK_ID_CURR\\n\\n', app.iloc[:, 111:115].head())\n",
    "print('\\nChild: bureau, Child Variable: SK_ID_CURR\\n\\n', bureau.iloc[10:30, :4].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SK_ID_CURR` \"100002\" has one row in the parent table and multiple rows in the child. \n",
    "\n",
    "Two tables are linked via a shared variable. The `app` and `bureau` dataframe are linked by the `SK_ID_CURR` variable while the `bureau` and `bureau_balance` dataframes are linked with the `SK_ID_BUREAU`. Defining the relationships is relatively straightforward, and the diagram provided by the competition is helpful for seeing the relationships. For each relationship, we need to specify the parent variable and the child variable. Altogether, there are a total of 6 relationships between the tables. Below we specify all six relationships and then add them to the EntitySet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between app and bureau\n",
    "r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n",
    "\n",
    "# Relationship between bureau and bureau balance\n",
    "r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "\n",
    "# Relationship between current app and previous apps\n",
    "r_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n",
    "\n",
    "# Relationships between previous apps and cash, installments, and credit\n",
    "r_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "r_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "r_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the defined relationships\n",
    "es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n",
    "                           r_previous_cash, r_previous_installments, r_previous_credit])\n",
    "# Print out the EntitySet\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly advanced note: we need to be careful to not create a [diamond graph](https://en.wikipedia.org/wiki/Diamond_graph) where there are multiple paths from a parent to a child. If we directly link `app` and `cash` via `SK_ID_CURR`; `previous` and `cash` via `SK_ID_PREV`; and `app` and `previous` via `SK_ID_CURR`, then we have created two paths from `app` to `cash`. This results in ambiguity, so the approach we have to take instead is to link `app` to `cash` through `previous`. We establish a relationship between `previous` (the parent) and `cash` (the child) using `SK_ID_PREV`. Then we establish a relationship between `app` (the parent) and `previous` (now the child) using `SK_ID_CURR`. Then featuretools will be able to create features on `app` derived from both `previous` and `cash` by stacking multiple primitives. (We removed the `SK_ID_CURR` from `cash`, `credit`, and `installments` so we didn't have to worry about doing this!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All entities in the entity can be related to each other. In theory this allows us to calculate features for any of the entities, but in practice, we will only calculate features for the `app` dataframe since that will be used for training/testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed Features\n",
    "\n",
    "Now is where featuretools is able to augment our domain knowledge. If we specify seed features, featuretools can then stack additional features on top of these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Features from bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_active = ft.Feature(es['bureau']['CREDIT_ACTIVE']) != 'Closed'\n",
    "credit_overdue = ft.Feature(es['bureau']['CREDIT_DAY_OVERDUE']) > 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Features from bureau balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_past_due = ft.Feature(es['bureau_balance']['STATUS']).isin(['1', '2', '3', '4', '5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Features from previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_not_approved = ft.Feature(es['previous']['NAME_CONTRACT_STATUS']) != 'Approved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Features from credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_past_due = ft.Feature(es['credit']['SK_DPD']) > 0.0\n",
    "credit_card_active = ft.Feature(es['credit']['NAME_CONTRACT_STATUS']) == 'Active'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Features from cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_active = ft.Feature(es['cash']['NAME_CONTRACT_STATUS']) == 'Active'\n",
    "cash_past_due = ft.Feature(es['cash']['SK_DPD']) > 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Features from installments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_late = ft.Feature(es['installments']['DAYS_ENTRY_PAYMENT']) > ft.Feature(es['installments']['DAYS_INSTALMENT'])\n",
    "installments_low_payment = ft.Feature(es['installments']['AMT_PAYMENT']) < ft.Feature(es['installments']['AMT_INSTALMENT']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_features = [installments_low_payment, installments_late,\n",
    "                 cash_past_due, cash_active,\n",
    "                 credit_card_active, credit_card_past_due, \n",
    "                 application_not_approved, balance_past_due,\n",
    "                 credit_overdue, credit_active]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Primitives\n",
    "\n",
    "A [feature primitive](https://docs.featuretools.com/automated_feature_engineering/primitives.html) is an operation applied to a table or a set of tables to create a feature. These represent simple calculations, many of which we already use in manual feature engineering, that can be stacked on top of each other to create complex features. Feature primitives fall into two categories:\n",
    "\n",
    "* __Aggregation__: function that groups together child datapoints for each parent and then calculates a statistic such as mean, min, max, or standard deviation. An example is calculating the maximum previous loan amount for each client. An aggregation works across multiple tables using relationships between tables.\n",
    "* __Transformation__: an operation applied to one or more columns in a single table. An example would be taking the absolute value of a column, or finding the difference between two columns in one table.\n",
    "\n",
    "A list of the available features primitives in featuretools can be viewed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the primitives in a dataframe\n",
    "primitives = ft.list_primitives()\n",
    "pd.options.display.max_colwidth = 100\n",
    "primitives[primitives['type'] == 'aggregation'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primitives[primitives['type'] == 'transform'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feature Synthesis\n",
    "\n",
    "Deep Feature Synthesis (DFS) is the process featuretools uses to make new features. DFS stacks feature primitives to form features with a \"depth\" equal to the number of primitives. For example, if we take the maximum value of a client's previous loans (say `MAX(previous.loan_amount)`), that is a \"deep feature\" with a depth of 1. To create a feature with a depth of two, we could stack primitives by taking the maximum value of a client's average montly payments per previous loan (such as `MAX(previous(MEAN(installments.payment)))`). The [original paper on automated feature engineering using deep feature synthesis](https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf) is worth a read. \n",
    "\n",
    "To perform DFS in featuretools, we use the `dfs`  function passing it an `entityset`, the `target_entity` (where we want to make the features), the `agg_primitives` to use, the `trans_primitives` to use and the `max_depth` of the features. Here we will use the default aggregation and transformation primitives,  a max depth of 2, and calculate primitives for the `app` entity. Because this process is computationally expensive, we can run the function using `features_only = True` to return only a list of the features and not calculate the features themselves. This can be useful to look at the resulting features before starting an extended computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFS with Default Primitives\n",
    "\n",
    "First, we can run deep feature synthesis with the default primitives and generate all of the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default primitives from featuretools\n",
    "default_agg_primitives =  [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\n",
    "default_trans_primitives =  [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"numwords\", \"characters\"]\n",
    "\n",
    "# DFS with specified primitives\n",
    "feature_names = ft.dfs(entityset = es, target_entity = 'app',\n",
    "                       trans_primitives = default_trans_primitives,\n",
    "                       agg_primitives=default_agg_primitives, \n",
    "                       max_depth = 2, n_jobs = -1, verbose = 1,\n",
    "                       features_only=True)\n",
    "\n",
    "print('%d Total Features' % len(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the primitives are not even used in the deep feature synthesis because they are not applicable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(feature_names[1])[10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "for feature in feature_names:\n",
    "    feature_list.append(str(feature)[10:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for primitive in default_agg_primitives:\n",
    "    included = False\n",
    "    for feature in feature_list:\n",
    "        if primitive.upper() in feature:\n",
    "            included = True\n",
    "    if not included:\n",
    "        print('{} not in features.'.format(primitive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for primitive in default_trans_primitives:\n",
    "    included = False\n",
    "    for feature in feature_list:\n",
    "        if ('%s(' % primitive.upper()) in feature:\n",
    "            included = True\n",
    "    if not included:\n",
    "        print('{} not in features.'.format(primitive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFS with seed features\n",
    "\n",
    "Now we can include the seed features and look at the number of features that will be created. We will select a few aggregation and transformation primitives to use rather than the entire default list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_primitives =  [\"sum\", \"max\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\n",
    "trans_primitives = ['percentile', 'and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = ft.dfs(entityset=es, target_entity='app',\n",
    "                       agg_primitives = agg_primitives,\n",
    "                       trans_primitives = trans_primitives,\n",
    "                       seed_features = seed_features,\n",
    "                       n_jobs = -1, verbose = 1, features_only = True,\n",
    "                       max_depth = 2)\n",
    "\n",
    "print(\"{} total features.\".format(len(feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Deep Feature Synthesis\n",
    "\n",
    "If we are content with the features that will be built, we can run deep feature synthesis and create the feature matrix. The following call runs the full deep feature synthesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(es) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "psutil.virtual_memory().total / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, feature_names = ft.dfs(entityset=es, target_entity='app',\n",
    "                                       agg_primitives = agg_primitives,\n",
    "                                       trans_primitives = trans_primitives,\n",
    "                                       seed_features = seed_features,\n",
    "                                       n_jobs = 1, verbose = 1, features_only = False,\n",
    "                                       max_depth = 2, chunk_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.reset_index(inplace = True)\n",
    "feature_matrix.to_csv('../../data/kaggle_home_credit/feature_matrix.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to implement automated feature engineering for a data science problem. Automated feature engineering allows us to create thousands of new features from a set of related data tables, aiding us immensely as data scientists. Moreover, we can still use domain knowledge in our features and even augment our domain knowledge by building on top of our own hand-built features. The benefits of automated feature engineering are significant and will considerably help us in our role as data scientists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
