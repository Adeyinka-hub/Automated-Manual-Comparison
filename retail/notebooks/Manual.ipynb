{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Feature Engineering on the Retail Dataset\n",
    "\n",
    "In this notebook we will work on manual feature engineering of a retail dataset. This dataset is originally from the UCI machine learning repository and is reminiscient of real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load in the raw data and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('../input/Online Retail.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_s3 = \"s3://featurelabs-static/online-retail-logs.csv\"\n",
    "altered_data = pd.read_csv(csv_s3, parse_dates=[\"order_date\"])\n",
    "altered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to understand what each of the columns represents. For that we can look at the [data description on the UCI Machine Learning Repository.](https://archive.ics.uci.edu/ml/datasets/online+retail#)\n",
    "\n",
    "* `InvoiceNo`: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n",
    "* `StockCode`: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. \n",
    "* `Description`: Product (item) name. Nominal. \n",
    "* `Quantity`: The quantities of each product (item) per transaction. Numeric.\t\n",
    "* `InvoiceDate`: Invice Date and time. Numeric, the day and time when each transaction was generated. \n",
    "* `UnitPrice`: Unit price. Numeric, Product price per unit in sterling. \n",
    "* `CustomerID`: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. \n",
    "* `Country`: Country name. Nominal, the name of the country where each customer resides.\n",
    "\n",
    "Based on these descriptions, we will rename the columns so they are easier to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['order_id', 'product_id', 'desc', 'quantity', \n",
    "                 'date', 'unit_price', 'customer_id', 'country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues: Duplicated Rows\n",
    "\n",
    "One problem we can quickly check for is duplicated rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = data[data.duplicated(keep = 'first')]\n",
    "print('There are {} duplicated rows.'.format(len(duplicated_rows)))\n",
    "duplicated_rows.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, because the customers may be wholesalers who then sell to their own customers, it is possible that the duplicates are legitimate. These customers may order two of the same product at the same time to sell to 2 of their own customers. Given the number of duplicated rows (about 1% of the data), it seems this is a common occurrence. For now, we will not drop the duplicated rows.\n",
    "\n",
    "## Issues: Cancelled Orders\n",
    "\n",
    "A second potential problem is that of cancelled orders. Any `order_id` beginning with a 'C' indicates an order that was cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['order_id'] = data['order_id'].astype(str)\n",
    "data['product_id'] = data['product_id'].astype(str)\n",
    "cancelled_orders = data[data['order_id'].str.startswith('C')]\n",
    "cancelled_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that the corresponding purchase cannot be found by just looking up the cancelled `order_id` without the 'C'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['order_id'] == '536391']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try to match on the `product_id`, absolute value of `quantity`, and `customer_id`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Matching on product_id, absolute value of quantity, and customer_id.')\n",
    "data[(data['product_id'] == '21983') & (data['quantity'] == 24) & (data['customer_id'] == 17548)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Matching on product_id and customer_id.')\n",
    "data[(data['product_id'] == '21983') & (data['customer_id'] == 17548)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get nothing! For this order, there is only the cancelled order but no corresponding purchase. The two options I see are:\n",
    "\n",
    "1. The purchase was not recorded in this data because it falls outside of the time frame.\n",
    "2. The purchase was made by a different customer than the customer who made the cancellation. \n",
    "\n",
    "The second option strikes me as unlikely because that would mean customers can cancel each other's orders. The first option appears more realistic: there are gaps in the data that don't cover all of the purchases. This is something that we just have to deal with in real-world data (unless we have the option to go and gather more data). \n",
    "\n",
    "Let's do another check of the data for any unusual `order_id`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unusual = data[data['order_id'].str.contains('^[A-Z]')]\n",
    "unusual[~unusual['order_id'].str.startswith('C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only other unusual information concerns three rows that represent: `Adjust bad debt`. These all have `NaN` for the customer, and since our primary focus will be on a customer level, we can remove these rows. In fact, we will remove any rows that have `NaN` for the customer id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nan_customers = np.sum(data['customer_id'].isnull())\n",
    "print(f'Dropping {n_nan_customers} rows.')\n",
    "data = data[data['customer_id'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That operation removed a considerable number of observations. However, because we are going by customers, if we can't identify the purchase with a customer, then we cannot use that information. We could use the information on a `Country` level basis, but that is not our concern at the moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Problem\n",
    "\n",
    "The first step we need to decide is what exactly we want to predict from this data. One question of interest to businesses is predicting the amount a customer will spend in the next 30 days. Customers predicted to spend more could potentially be of higher interest - meaning more time should be spent advertisting products to them - or this information could be used to form customers \"groupings\" allowing for more efficient marketing. We'll formulate our problem as trying to predict __how much will a customer spend in the next 30 days.__\n",
    "\n",
    "To do this, our first step is to identify the last date in the data for each customer. We'll then go forward 30 days in the data to establish a prediction point (known as a cutoff time). The target will be the total amoung of spending between the cutoff point and the last date in the data and we cannot use any information past the cutoff point for predicting the target. Because we want to know the total price spent (in Sterling), we'll multiple the `quantity` by the `unit_price`. Then we can form the targets and subset the data to that which is usable for each customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_price'] = data['quantity'] * data['unit_price']\n",
    "n_nan_price = np.sum(data['total_price'].isnull())\n",
    "print(f'Removing {n_nan_price} rows with unknown total prices.')\n",
    "data = data[data['total_price'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to forming the labels, it's helpful to graph the distribution of prices to see if there are any outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(8, 6)\n",
    "\n",
    "# KDE plot of total price\n",
    "sns.distplot(data['total_price'], hist=False, color = 'red');\n",
    "plt.title('Total Purchase Price Distribution');\n",
    "plt.xlabel('Purchase Price'); plt.ylabel('Density');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be two issues: 1. There are negative purchase prices, and 2. There are extreme outliers on both the upper and lower limit.\n",
    "\n",
    "The first problem makes sense if we think about the cancellations, which we left in the data. These appear as negative quantities and hence the `total_price` would be negative. However, this is actually not an issue if our goal is to predict total spending. A negative purchase offsets a positive purchase and so even though it might seem odd at first, leaving these cancellations in will allow us to incorporate all of the information about a customer. \n",
    "\n",
    "The outliers on both the high and the low end should also be left in because we have no reason to believe these are false. To investigate, we can look at both the highest and lowest total purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['total_price'].idxmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['total_price'].idxmin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest total purchase price is for an order that was eventualy cancelled (12 minutes later!). This means that for this customer, the total of these two orders would come out to \\$0.0. \n",
    "\n",
    "The only potential issue with leaving in the cancellations is for the edge cases where the purchase occurred before the cutoff point but the cancellation was after the cutoff point which could mess with the data. However, we'll not worry about this low probability event for the moment.\n",
    "\n",
    "Now, back to making the prediction problem. The lines below establish a cutoff dataframe that we'll use for building the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = pd.DataFrame(data.groupby(['customer_id'])['date'].max())\n",
    "problem['cutoff_point'] = [date - pd.Timedelta(30, unit = 'd') for date in problem['date']]\n",
    "problem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to iterate through each customer and find the total spending in the last month of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = []\n",
    "\n",
    "# Iterate through each customer\n",
    "for customer_id, customer_info in problem.iterrows():\n",
    "    cutoff_point = customer_info['cutoff_point']\n",
    "    \n",
    "    # Subset the data to after the prediction point\n",
    "    subset = data.loc[(data['date'] > cutoff_point) & (data['customer_id'] == customer_id), :].copy()\n",
    "    \n",
    "    # Add the number of unique orders in the month to a list\n",
    "    totals.append(subset['total_price'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the labels seem reasonable, we should calculate the statistics and show a visual or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem['target'] = totals\n",
    "problem['target'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do end up with some outliers. However, we'll have to live with these discrepancies. We can remove the customers with a negative label because we want to focus on customers that can generate revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = problem[problem['target'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10**(np.arange(0,5))\n",
    "p = plt.hist(problem['target'], bins = [1, 10, 100, 100, 1000, 10000, 100000] , \n",
    "             edgecolor = 'k')\n",
    "plt.xscale('log'); plt.xlabel('Total Purchase ($)'); plt.ylabel('Count'); plt.title('Total Purchase in Last Month');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of purchases fall in between \\$100 and \\$1000. We can also view the data with an empirical cumulative distribution function plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(a):\n",
    "    x = np.sort(a)\n",
    "    y = np.arange(len(x)) / float(len(x))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ecdf(problem['target'])\n",
    "plt.plot(x, y);\n",
    "plt.xlabel('Total Purchase'); plt.ylabel('Percentile'); plt.title('ECDF of Target');\n",
    "print(f'The 90th percentile is at ${x[np.where(y > 0.9)[0][0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Data to before the Prediction Point\n",
    "\n",
    "The cutoff date represents the last time we can use information in the data for that customer to train a model for our prediction problem. We now need to use these cutoff dates to limit the data to that which is allowable. For each customer, that means subsetting the data to before the `cutoff_point`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_data = pd.DataFrame(columns = data.columns)\n",
    "\n",
    "# Iterate through each customer\n",
    "for customer_id, row in problem.iterrows():\n",
    "\n",
    "    # Subset the data to before the cutoff point.\n",
    "    subset = data[(data['customer_id'] == customer_id) & (data['date'] < row['cutoff_point'])]\n",
    "    usable_data = usable_data.append(subset, ignore_index = True, sort = False)\n",
    "    \n",
    "usable_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(usable_data)} observations in the final data with {usable_data[\"customer_id\"].nunique()} unique customers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = usable_data.copy()\n",
    "usable_data.to_csv('../input/usable_purchases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Sets\n",
    "\n",
    "At this point we want to split the data into training and testing sets and then never look at the testing data until we have to evaluate the final model. We can use a straightforward train/test split with 30% of the data used for testing. We'll split based on the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_customers, test_customers, train_labels, test_labels = train_test_split(problem.index, problem['target'], \n",
    "                                                                             test_size = 0.3, random_state = 50)\n",
    "\n",
    "print(f'{len(train_customers)} training customers and {len(test_customers)} testing customers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We know the data we can use and we have a prediction problem: estimate the amount a customer will spend in the next 30 days. Now we need to get the data into a usable format for a machine learning model to train. Because we are predicting on a customer level basis, this means one table where every row is a customer and the columns are the features for that customer. Right now we have 0 features per customer but plenty of data of their past purchases. We can use the purchase data to create features, such as the total previous amount spent, or the most popular day of the week on which they make purchases.\n",
    "\n",
    "Feature engineering refers to this process of making features for a machine learning model out of a dataset. The next section of this notebook will be an implementation of all the steps required for manual feature engineering. This requires building each feature one at a time, relying on domain knowledge to construct predictors that are relevant to the domain. This can be a tedious, time-consuming, error-prone process, and is not transferable between problems because the features are very specific. In a later notebook, we will see a more efficient approach, automated feature engineering using Featuretools. \n",
    "\n",
    "### Adding More Information\n",
    "\n",
    "One of the simplest feature engineering steps is to extract information from the original data such as by finding the `weekday`, `month`, `hour`, and `minute` of the purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekday'] = data['date'].dt.weekday\n",
    "data['month'] = data['date'].dt.month\n",
    "data['minute'] = data['date'].dt.minute\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['date_only'] = data['date'].dt.date\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekday'].value_counts().sort_index().plot.bar(color = 'orange');\n",
    "plt.title('Purchases by Day of Week'); plt.ylabel('Purchases'); plt.xlabel('Day of Week');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hour'].value_counts().sort_index().plot.bar(color = 'orange');\n",
    "plt.title('Purchases by Time of Day'); plt.ylabel('Purchases'); plt.xlabel('Hour of Day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(20, 6)\n",
    "trends = data.groupby('date_only')['total_price'].sum()\n",
    "trends.plot()\n",
    "plt.title('Total Purchase Amount over Time'); plt.xlabel(''); plt.ylabel('Total Purchase Amount');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Data\n",
    "\n",
    "The first step we need to take is take the one table and break it into discrete normalized tables. We'll create 4 tables of information:\n",
    "\n",
    "* customers: each customer (`customter_id`) will have one row\n",
    "* products: each product (`product_id`) will have one row\n",
    "* orders: each invoice (`order_id`) will have one row\n",
    "* purchases: each purchased item will have one row\n",
    "\n",
    "The purchases dataframe is the child of all the other dataframes. The `customers` dataframe is where we will make our features for the prediction problem. Hence to make features, we will have to aggregate the purchase data for each customer. Let's start by breaking the data into multiple dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orders\n",
    "\n",
    "First we'll create a table of just the orders. Each order is uniquely identified by the `order_id` and also has a date, customer, and country. We can also add a `total_price` for the order by summing up all of the products associated with that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.DataFrame(data.groupby(['order_id', 'date', 'customer_id', 'country'])['total_price'].sum().reset_index())\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should make sure that this table is normalized by checking that there are no duplicated order ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {orders[\"order_id\"].nunique()} unique orders and {np.sum(orders[\"order_id\"].duplicated())} duplicated orders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us we made an error in creating the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders[orders.duplicated(subset = 'order_id', keep = False)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that orders do not all have the same date. Therefore when we made the orders table, we should not have grouped by the date time. However, we can group by just the `date_only` and see if all orders are made on the same day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.DataFrame(data.groupby(['order_id', 'date_only', 'customer_id', 'country'])['total_price'].sum().reset_index())\n",
    "print(f'There are {orders[\"order_id\"].nunique()} unique orders and {np.sum(orders[\"order_id\"].duplicated())} duplicated orders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are good to go without the duplicated orders. We can use this table to create features, for example by finding the total number of orders of each customer or the most common day on which they make purchases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customers\n",
    "\n",
    "Each customer is uniquely identified by the `customer_id`. This table will be where we make all the features since we are predicting based on each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.DataFrame(problem.index.unique())\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(customers['customer_id'].duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products\n",
    "\n",
    "Each product is identified by the `product_id`. The unit price might not stay the same so we cannot associate this with each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.DataFrame(data['product_id'].unique(), columns = ['product_id'])\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchases\n",
    "\n",
    "Each purchase is idenitifed by combination of the `order_id`, the `customer_id`, and the `product_id`. The purchases data is already saved as the data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "Before we get started with the real feature engineering and modeling, we should establish a naive baseline model. This will let us measure the effectiveness of any feature engineering. For a regression problem, a naive baseline can be simply the average value of the train labels. \n",
    "\n",
    "## Metric \n",
    "\n",
    "Since this is a regression problem dealing with a quantity that has a real meaning, we want to use a metric that is easily understandable. The __Median Absolute Error__ (MAE) for this problem shows the median dollar amount that our forecasts are off. This can be calculated as simply the median of the absolute value of the prediction minus the true value:\n",
    "\n",
    "$$\\text{MAE} = \\text{median(abs(prediction - true value))}$$\n",
    "\n",
    "The median absolute error is a better metric than the mean absolute error since the mean will be unduly influenced by outliers (and we saw that there are a number of outliers in the target).\n",
    "\n",
    "### Naive Baseline\n",
    "\n",
    "Below we establish a naive baseline by guessing the value of the test label is the average value on the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "print(f'Baseline Guess Average Error = ${round(median_absolute_error(test_labels, [np.mean(train_labels) for _ in range(len(test_labels))]), 2)}.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Informed Baseline\n",
    "\n",
    "A slightly better estimate would be to guess that customers will spend as much in the next month as they spent in the previous month. We can figure this out by using the last thirty days of the purchase data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirty_day_totals = []\n",
    "\n",
    "# Iterate through each customer\n",
    "for customer_id, customer_info in problem[problem.index.isin(test_customers)].iterrows():\n",
    "    \n",
    "    cutoff_point = customer_info['date'] - pd.Timedelta(30, 'D')\n",
    "    \n",
    "    # Subset the data to after the prediction point\n",
    "    subset = data.loc[(data['date'] > cutoff_point) & (data['customer_id'] == customer_id), :].copy()\n",
    "    \n",
    "    # Add the number of unique orders in the month to a list\n",
    "    thirty_day_totals.append(subset['total_price'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Informed Baseline Average Error: ${round(median_absolute_error(test_labels, thirty_day_totals), 2)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The informed baseline significantly outperformed the naive baseline. Now we know that if we can't predict with a median absolute error less than around \\$288, then maybe machine learning is not applicable to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Implementation\n",
    "\n",
    "After we established our baseline, we can start making features for a machine learning model to learn the problem. In manual feature engineering, we build features one at a time. We can constantly evaluate these features using cross-validation on the training set. \n",
    "\n",
    "## Building Features from Previous Orders\n",
    "\n",
    "The first set of features will be built on the previous orders. We can find the __total__ number of a customers previous orders, the __total value__ of a customers previous orders, and then also divide both of these by the time between the __first order and last order__ in order to get an orders / time feature.\n",
    "\n",
    "Below we find the total orders and total value of orders for customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_orders = orders.groupby('customer_id')['order_id'].count()\n",
    "total_value_orders = orders.groupby('customer_id')['total_price'].sum()\n",
    "totals = pd.concat([total_orders, total_value_orders], axis = 1).rename(columns = {'order_id': 'num_orders',\n",
    "                                                                                   'total_price': 'total_spending'})\n",
    "totals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(8, 6)\n",
    "sns.lmplot('num_orders', 'total_spending', data = totals, size = 6)\n",
    "plt.title('Total Spending versus Number of Orders');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time a customer has been in the records, we'll take the last order data minus the first order date and convert it to the number of months. Then we can divide the `num_orders` and `total_spending` by this figure to arrive at a final ratio per 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find timedelta between last and first order\n",
    "totals['record_length'] = orders.groupby('customer_id')['date_only'].apply(lambda x: x.max() - x.min())\n",
    "\n",
    "# Convert to 30 days\n",
    "totals['record_length'] = totals['record_length'].dt.total_seconds() / (3600 * 30)\n",
    "\n",
    "# Get ratios\n",
    "totals['num_orders_ratio'] = totals['num_orders'] / totals['record_length']\n",
    "totals['spending_ratio'] = totals['total_spending'] / totals['record_length']\n",
    "totals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to intrepret these numbers: what the `spending_ratio` indicates is the total spending for every 30 days, and the `num_orders_ratio` is the number of orders on average every 30 days. \n",
    "\n",
    "We'll replace the `inf` with `np.nan` which later will let us impute these values before sending the data to a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = totals.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "sns.violinplot(y = totals['spending_ratio'].dropna());\n",
    "plt.title('Violin Plot of Spending Ratio');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals.loc[totals['spending_ratio'].idxmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest spending in the data is from a customer that averages over \\$10000 per month, because they are have been in the data for less than one month yet have a total spending of \\$8000. As a final feature, we can calculate the spending per order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals['spending_per_order'] = totals['total_spending'] / totals['num_orders']\n",
    "sns.kdeplot(totals['spending_per_order'])\n",
    "plt.title('Spending per Order Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Feature Set\n",
    "\n",
    "From just these 5 features we can already train a model (we only need a single feature to train a model). We'll use a random forest regressor as implemented in the `utils.py` module and the `evaulate` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import evaluate, plot_feature_importances, feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = totals[totals.index.isin(train_customers)]\n",
    "test = totals[totals.index.isin(test_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, fi = evaluate(train, train_labels, test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already our model has outperformed the baseline! This means machine learning is applicable to the problem. To improve our performance, we want to keep adding more features derived from the data. We can use the feature importances from the first model to see which features are considered most relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fi = plot_feature_importances(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = train_labels\n",
    "train.to_csv('../input/train_features.csv')\n",
    "test['target'] = test_labels\n",
    "test.to_csv('../input/test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature is the `total_spending`. This tells us that how much a customer has spent total in the past is indicative of the amount they will spend in the next 30 days. \n",
    "\n",
    "### More Features from the Order Table\n",
    "\n",
    "We'll continue with feature engineering using the orders table. We'll make an additional 4 features:\n",
    "\n",
    "* Spending in the last 30 days of data\n",
    "* Number of orders in the last 30 days of data\n",
    "* Spending per order in the last 30 days of data\n",
    "* Most common country ordered from\n",
    "* Number of unique countries ordered from\n",
    "\n",
    "Although the baseline guess using the spending in the previous 30 days did not prove helpful, perhaps it will be of some use to a machine learning model. Since the spending per order was an important feature, we also will want to calculate the spending per order in the past 30 days. This shows the iterative process of feature engineering: we can use previous results to inform future decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirty_day_orders = []\n",
    "thirty_day_totals = []\n",
    "\n",
    "for customer_id, customer_info in problem.iterrows():\n",
    "\n",
    "    # Identify the 30 day mark for each customer\n",
    "    cutoff_point = (customer_info['cutoff_point'] - pd.Timedelta(30, 'D')).date()\n",
    "    \n",
    "    # Subset to the last thirty days\n",
    "    subset = orders[(orders['customer_id'] == customer_id) & (orders['date_only'] > cutoff_point)].copy()\n",
    "    thirty_day_orders.append(subset.shape[0])\n",
    "    thirty_day_totals.append(subset['total_price'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals['thirty_day_orders'] = thirty_day_orders\n",
    "totals['thirty_day_totals'] = thirty_day_totals\n",
    "totals['thirty_day_spending_per_order'] = totals['thirty_day_totals'] / totals['thirty_day_orders']\n",
    "\n",
    "plt.figure(figsize = (12, 6))\n",
    "sns.kdeplot(totals['thirty_day_spending_per_order']);\n",
    "plt.title('Thirty Day Spending per Order Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see a positively skewed distribution with most customers spending per order below \\$1000 in the previous 30 days, but a few customers with spending per order much higher. \n",
    "\n",
    "The next two features will deal with the `country` of the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_countries = orders.groupby('customer_id')['country'].nunique()\n",
    "most_common_country = orders.groupby('customer_id')['country'].apply(lambda x: pd.Series.mode(x)[0])\n",
    "\n",
    "totals['num_unique_countries'] = unique_countries\n",
    "totals['most_common_country'] = most_common_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "g = sns.boxplot(x = 'most_common_country', y = 'thirty_day_spending_per_order', data = totals)\n",
    "loc, labels = plt.xticks()\n",
    "g.set_xticklabels(labels, rotation=75)\n",
    "plt.title('Thirty Day Spending per Order by Country');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to one-hot encode the country in order to pass it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = pd.get_dummies(totals)\n",
    "totals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate this model to see if the 5 (more if we count the one-hot encoding) extra features improved the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = totals[totals.index.isin(train_customers)]\n",
    "test = totals[totals.index.isin(test_customers)]\n",
    "\n",
    "preds, fi = evaluate(train, train_labels, test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fi = plot_feature_importances(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the new features did not improve the model even though the most important feature was one of those that we added. It looks like the country feature are not important. Most of the customers (~90%) are from the United Kingdom so there is not much information content in the country of the order.\n",
    "\n",
    "At this point we have extracted a lot of information from the `orders` table, but there is plenty more that we did not consider such as using the day of week, the month of year, or creating country level features and then applying these to each customer. With manual feature engineering, __we are limited by our time and creativity__. There are only so many features that we can make by hand, one of the fundamental issues with the traditional approach to feature engineering. \n",
    "\n",
    "### Features from the Purchase Data\n",
    "\n",
    "Next we'll work of building features from the purchase level data. This gives us more granular information than the `orders` and presents us with even more potential features to make. We'll start off by finding the total number of purchases, the sum of purchases (which should be the same as that from the orders) and the ratios of these per 30 day period, much as we did with the orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_purchases = purchases.groupby('customer_id')['order_id'].count()\n",
    "total_value_purchases = purchases.groupby('customer_id')['total_price'].sum()\n",
    "total_purchases = pd.concat([total_purchases, total_value_purchases], axis = 1).rename(columns = {'order_id': 'num_purchases',\n",
    "                                                                                                  'total_price': 'total_purchase_spending'})\n",
    "total_purchases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find timedelta between last and first order\n",
    "total_purchases['record_length'] = purchases.groupby('customer_id')['date'].apply(lambda x: x.max() - x.min())\n",
    "\n",
    "# Convert to 30 days\n",
    "total_purchases['record_length'] = total_purchases['record_length'].dt.total_seconds() / (3600 * 30)\n",
    "\n",
    "# Get ratios\n",
    "total_purchases['num_purchases_ratio'] = total_purchases['num_purchases'] / total_purchases['record_length']\n",
    "total_purchases['spending_ratio'] = total_purchases['total_purchase_spending'] / total_purchases['record_length']\n",
    "total_purchases = total_purchases.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "sns.violinplot(y = total_purchases['num_purchases'])\n",
    "plt.title('Number of Purchases');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('record_length', 'num_purchases', data = total_purchases, size = 8);\n",
    "plt.title('Number of Purchases vs 30 Day Periods');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = totals.merge(total_purchases, on = 'customer_id')\n",
    "totals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = totals[totals.index.isin(train_customers)]\n",
    "test = totals[totals.index.isin(test_customers)]\n",
    "\n",
    "preds, fi = evaluate(train, train_labels, test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model performance has now decreased significantly. One issue is that we could have too many features: some of the features could be correlated with one another which can lead to worse performance. We can use the `feature_selection` function from `utils.py` to remove potentially problematic features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_selection(train, missing_threshold=90, correlation_threshold=0.9)\n",
    "test = test[train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, fi = evaluate(train, train_labels, test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection did not help which is somewhat surprising. It's possible that we accidentally removed some features that were critical to the model. Let's try one last round of feature engineering. We'll load back in the features that have performed the best and try to add some more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train_features.csv')\n",
    "test = pd.read_csv('../input/test_features.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Round of Feature Engineering \n",
    "\n",
    "For the last round of feature engineering we'll focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_purchases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second baseline will be made by applying a Random Forest Regressor to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data['total_price'] = (data['quantity'] * data['unit_price']).astype(np.float32)\n",
    "sns.kdeplot(data['total_price']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several negative total prices which are not physically possible. If we look at the original data, this is because there are several orders that were cancelled (identifed by a 'C' preceding the order id). To handle these, we'll remove the cancelled lines themselves but add a flag indicating the order was cancelled.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['order_id'] = data['order_id'].astype(str)\n",
    "cancelled_order_ids = [x[1:] for x in data['order_id'] if x.startswith('C')]\n",
    "len(cancelled_order_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cancelled'] = data['order_id'].isin(cancelled_order_ids)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['order_id'].str.startswith('C')]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll set to 0 any of the total value where the order was cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['cancelled'], 'total_price'] = np.nan\n",
    "sns.kdeplot(data['total_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cancelled orders are not duplicated in the orders table but are instead identified by a 'C' in front of the order id. We'll add a flag indicating if the order was cancelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['order_id'] = orders['order_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['cancelled'] = orders['order_id'].str.startswith('C')\n",
    "orders['cancelled'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a feature for customers, we can count the number of cancelled orders and the number of non-cancelled orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled = pd.DataFrame(orders.groupby('customer_id')['cancelled'].sum())\n",
    "total = pd.DataFrame(orders.groupby('customer_id')['cancelled'].count()).rename(columns = {'cancelled': 'total'})\n",
    "customer_orders = cancelled.merge(total, on = 'customer_id')\n",
    "customer_orders['non_cancelled'] = customer_orders['total'] - customer_orders['cancelled']\n",
    "customer_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_orders.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to build a model from just this information and see if it is at all useful! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(customer_orders, customers['label'],\n",
    "                                        stratify = customers['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "scorer = make_scorer(f1_score, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = data.groupby('customer_id')['date'].apply(lambda x: x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(model, X, y, scoring = scorer, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "f1_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ecdf(data['total_price'])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['total_price'] < 0) | (data['total_price'] > 10000), 'total_price'] = np.nan\n",
    "sns.kdeplot(data['total_price']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weekday'] = data['date'].dt.weekday\n",
    "data['month'] = data['date'].dt.month\n",
    "data['minute'] = data['date'].dt.minute\n",
    "data['hour'] = data['date'].dt.hour\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start making features for each customer. As an example, we can count the number of previous orders by the customer. Then, we can divide the number of previous orders by the date between the first and last occurrence of the customer in the data to come up with a rate of purchases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_limits = usable_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = orders.merge(data, on = 'order_id', how = 'left'a\n",
    "                     )\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_data = ft.demo.load_retail()\n",
    "ft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
